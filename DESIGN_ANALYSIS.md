# Python OpenMessaging Benchmark - è®¾è®¡åˆ†æä¸æ”¹è¿›å»ºè®®

## æ–‡æ¡£ä¿¡æ¯

- **é¡¹ç›®**: py-openmessaging-benchmark
- **åˆ†ææ—¥æœŸ**: 2025-09-30
- **å¯¹æ¯”åŸºå‡†**: Java OpenMessaging Benchmark (OMB)
- **å½“å‰ç‰ˆæœ¬**: 0.1.0

---

## ä¸€ã€å½“å‰è®¾è®¡ä¸å®ç°æ€»ç»“

### 1.1 æ¶æ„æ¦‚è§ˆ

#### æ•´ä½“æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Coordinator                          â”‚
â”‚  - æµ‹è¯•ç¼–æ’                                                  â”‚
â”‚  - ä»»åŠ¡åˆ†å‘                                                  â”‚
â”‚  - ç»“æœèšåˆ                                                  â”‚
â”‚  - HTTP Client (aiohttp)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                â”‚
             â”‚ REST API (HTTP)                â”‚
             â”‚                                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Worker 1      â”‚              â”‚   Worker 2      â”‚
    â”‚  - FastAPI      â”‚              â”‚  - FastAPI      â”‚
    â”‚  - Producerè¿æ¥æ± â”‚              â”‚  - Producerè¿æ¥æ± â”‚
    â”‚  - ä»»åŠ¡æ‰§è¡Œå™¨    â”‚              â”‚  - ä»»åŠ¡æ‰§è¡Œå™¨    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                â”‚
             â”‚                                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Kafka Cluster                       â”‚
    â”‚  - Topics (åŠ¨æ€åˆ›å»º/åˆ é™¤)                        â”‚
    â”‚  - Partitions                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### æ¨¡å—ç»“æ„
```
py-openmessaging-benchmark/
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ coordinator.py         # åè°ƒå™¨ (393 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ worker.py              # Worker æŠ½è±¡åŸºç±» (244 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ config.py              # é…ç½®ç®¡ç† (156 è¡Œ)
â”‚   â”‚   â”œâ”€â”€ results.py             # ç»“æœæ”¶é›†ä¸ç»Ÿè®¡ (328 è¡Œ)
â”‚   â”‚   â””â”€â”€ monitoring.py          # ç³»ç»Ÿç›‘æ§
â”‚   â”œâ”€â”€ drivers/                   # é©±åŠ¨æŠ½è±¡å±‚
â”‚   â”‚   â”œâ”€â”€ base.py                # æŠ½è±¡æ¥å£å®šä¹‰ (367 è¡Œ)
â”‚   â”‚   â””â”€â”€ kafka/                 # Kafka é©±åŠ¨å®ç°
â”‚   â”‚       â”œâ”€â”€ kafka_driver.py    # Kafka é©±åŠ¨å…¥å£ (115 è¡Œ)
â”‚   â”‚       â”œâ”€â”€ kafka_producer.py  # ç”Ÿäº§è€…å®ç°
â”‚   â”‚       â”œâ”€â”€ kafka_consumer.py  # æ¶ˆè´¹è€…å®ç°
â”‚   â”‚       â””â”€â”€ kafka_topic_manager.py  # Topic ç®¡ç†
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ worker_api.py          # FastAPI Worker æ¥å£ (248 è¡Œ)
â”‚   â””â”€â”€ utils/                     # å·¥å…·ç±»
â”‚       â”œâ”€â”€ rate_limiter.py        # é€Ÿç‡é™åˆ¶å™¨
â”‚       â”œâ”€â”€ latency_recorder.py    # å»¶è¿Ÿè®°å½•å™¨
â”‚       â””â”€â”€ parallel_sender.py     # å¤šè¿›ç¨‹å‘é€å™¨
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ kafka_worker.py            # Kafka Worker å®ç° (678 è¡Œ)
â”œâ”€â”€ configs/                       # é©±åŠ¨é…ç½®
â”‚   â””â”€â”€ kafka-*.yaml               # Kafka é…ç½®æ–‡ä»¶
â””â”€â”€ workloads/                     # å·¥ä½œè´Ÿè½½å®šä¹‰
    â””â”€â”€ *.yaml                     # 18 ä¸ªé¢„å®šä¹‰æµ‹è¯•åœºæ™¯
```

### 1.2 æ ¸å¿ƒè®¾è®¡ç‰¹æ€§

#### 1.2.1 åˆ†å¸ƒå¼ Coordinator-Worker æ¶æ„

**Coordinator èŒè´£**:
- è§£æå·¥ä½œè´Ÿè½½é…ç½® (YAML)
- æ£€æŸ¥ Worker å¥åº·çŠ¶æ€ (`/health` ç«¯ç‚¹)
- ç”Ÿæˆå¹¶åˆ†å‘ Producer/Consumer ä»»åŠ¡
- åè°ƒæµ‹è¯•æ—¶åº (Warmup â†’ Main Test)
- æ”¶é›†å¹¶èšåˆ Worker ç»“æœ
- ç®¡ç† Topic ç”Ÿå‘½å‘¨æœŸ (åˆ›å»º/åˆ é™¤)

**Worker èŒè´£**:
- æä¾› REST API æ¥å£ (FastAPI)
- æ‰§è¡Œå…·ä½“çš„ Producer/Consumer ä»»åŠ¡
- ç»´æŠ¤æ¶ˆæ¯ç³»ç»Ÿè¿æ¥ (è¿æ¥æ± )
- æ”¶é›†æ€§èƒ½æŒ‡æ ‡ (ååé‡ã€å»¶è¿Ÿã€é”™è¯¯)
- ç³»ç»Ÿèµ„æºç›‘æ§ (CPUã€å†…å­˜ã€ç½‘ç»œ)

**é€šä¿¡åè®®**:
```http
POST /producer/start       # å¯åŠ¨ç”Ÿäº§è€…ä»»åŠ¡
POST /consumer/start       # å¯åŠ¨æ¶ˆè´¹è€…ä»»åŠ¡
GET  /health               # å¥åº·æ£€æŸ¥
GET  /task/{id}/status     # ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢
GET  /task/{id}/result     # ä»»åŠ¡ç»“æœè·å–
```

#### 1.2.2 å¯æ’æ‹”é©±åŠ¨ç³»ç»Ÿ

**æŠ½è±¡æ¥å£è®¾è®¡**:
```python
AbstractDriver
â”œâ”€â”€ initialize()           # åˆå§‹åŒ–é©±åŠ¨
â”œâ”€â”€ cleanup()              # æ¸…ç†èµ„æº
â”œâ”€â”€ create_producer()      # åˆ›å»ºç”Ÿäº§è€…
â”œâ”€â”€ create_consumer()      # åˆ›å»ºæ¶ˆè´¹è€…
â””â”€â”€ create_topic_manager() # åˆ›å»º Topic ç®¡ç†å™¨

AbstractProducer
â”œâ”€â”€ send_message()         # å‘é€å•æ¡æ¶ˆæ¯
â”œâ”€â”€ send_batch()           # æ‰¹é‡å‘é€
â”œâ”€â”€ flush()                # åˆ·æ–°ç¼“å†²
â””â”€â”€ close()                # å…³é—­è¿æ¥

AbstractConsumer
â”œâ”€â”€ subscribe()            # è®¢é˜… Topic
â”œâ”€â”€ consume_messages()     # æ¶ˆè´¹æ¶ˆæ¯ (AsyncIterator)
â”œâ”€â”€ commit()               # æäº¤ Offset
â””â”€â”€ close()                # å…³é—­è¿æ¥

AbstractTopicManager
â”œâ”€â”€ create_topic()         # åˆ›å»º Topic
â”œâ”€â”€ delete_topic()         # åˆ é™¤ Topic
â”œâ”€â”€ list_topics()          # åˆ—å‡º Topic
â””â”€â”€ topic_exists()         # æ£€æŸ¥ Topic æ˜¯å¦å­˜åœ¨
```

**å½“å‰å®ç°**:
- Kafka é©±åŠ¨ (åŸºäº confluent-kafka)
- æ”¯æŒåŠ¨æ€åŠ è½½ (é…ç½®ä¸­æŒ‡å®š `driverClass`)

#### 1.2.3 æµ‹è¯•æ‰§è¡Œæµç¨‹

**å®Œæ•´æµç¨‹** (`coordinator.py:40-136`):
```python
1. é…ç½®éªŒè¯ä¸åŠ è½½
   â”œâ”€â”€ åŠ è½½ Workload é…ç½® (topics, producers, consumers, duration)
   â”œâ”€â”€ åŠ è½½ Driver é…ç½® (Kafka è¿æ¥ã€ä¼˜åŒ–å‚æ•°)
   â””â”€â”€ éªŒè¯é…ç½®åˆæ³•æ€§ (validate_driver_config/validate_workload_config)

2. ç¯å¢ƒå‡†å¤‡
   â”œâ”€â”€ æ£€æŸ¥æ‰€æœ‰ Worker å¥åº·çŠ¶æ€
   â”œâ”€â”€ ç”Ÿæˆå”¯ä¸€æµ‹è¯• ID (test_name_timestamp)
   â”œâ”€â”€ åˆ›å»ºå”¯ä¸€ Topic åç§° (benchmark-{test_id}-topic-{idx})
   â””â”€â”€ åˆ›å»º Topics (å¹‚ç­‰æ€§è®¾è®¡)

3. Warmup é˜¶æ®µ (å¯é€‰)
   â”œâ”€â”€ ä½¿ç”¨ç›¸åŒé…ç½®é¢„çƒ­ç³»ç»Ÿ
   â”œâ”€â”€ æŒç»­ warmup_duration_minutes
   â””â”€â”€ ä¸¢å¼ƒé¢„çƒ­ç»“æœ (ä»…è®°å½•æ—¥å¿—)

4. ä¸»æµ‹è¯•é˜¶æ®µ
   â”œâ”€â”€ ç”Ÿæˆ Producer ä»»åŠ¡
   â”‚   â”œâ”€â”€ æ¯ä¸ª Topic Ã— producers_per_topic = æ€» Producer æ•°
   â”‚   â”œâ”€â”€ è®¡ç®—æ¯ä¸ª Producer çš„æ¶ˆæ¯é…é¢
   â”‚   â””â”€â”€ è®¾ç½®é€Ÿç‡é™åˆ¶ (rate_limit)
   â”‚
   â”œâ”€â”€ ç”Ÿæˆ Consumer ä»»åŠ¡
   â”‚   â”œâ”€â”€ æ¯ä¸ª Topic Ã— subscriptions_per_topic Ã— consumers_per_subscription
   â”‚   â”œâ”€â”€ ç”Ÿæˆå”¯ä¸€è®¢é˜…å (subscription-{idx}-{test_id})
   â”‚   â””â”€â”€ è®¾ç½®æµ‹è¯•æŒç»­æ—¶é—´
   â”‚
   â”œâ”€â”€ ä»»åŠ¡åˆ†å‘ç­–ç•¥
   â”‚   â”œâ”€â”€ Round-robin åˆ†é…ç»™ Workers
   â”‚   â””â”€â”€ è´Ÿè½½å‡è¡¡è€ƒè™‘
   â”‚
   â”œâ”€â”€ æ‰§è¡Œé¡ºåº (å…³é”®æ—¶åº)
   â”‚   â”œâ”€â”€ 1) å¯åŠ¨æ‰€æœ‰ Consumer ä»»åŠ¡
   â”‚   â”œâ”€â”€ 2) ç­‰å¾… 5 ç§’ (ç­‰å¾…è®¢é˜…å®Œæˆ)
   â”‚   â”œâ”€â”€ 3) å¯åŠ¨æ‰€æœ‰ Producer ä»»åŠ¡
   â”‚   â”œâ”€â”€ 4) ç­‰å¾… Producer å®Œæˆ
   â”‚   â””â”€â”€ 5) ç­‰å¾… Consumer å®Œæˆ
   â”‚
   â””â”€â”€ ç»“æœæ”¶é›†
       â”œâ”€â”€ æ”¶é›†æ‰€æœ‰ Producer ç»“æœ
       â”œâ”€â”€ æ”¶é›†æ‰€æœ‰ Consumer ç»“æœ
       â””â”€â”€ è®°å½•ç³»ç»Ÿç›‘æ§æ•°æ®

5. ç»“æœèšåˆ
   â”œâ”€â”€ èšåˆ Producer ååé‡ (æ±‚å’Œ)
   â”œâ”€â”€ èšåˆ Consumer ååé‡ (æ±‚å’Œ)
   â”œâ”€â”€ èšåˆå»¶è¿Ÿç»Ÿè®¡ (åŠ æƒå¹³å‡ + åˆ†ä½æ•°)
   â””â”€â”€ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

6. æ¸…ç†é˜¶æ®µ
   â”œâ”€â”€ åˆ é™¤æµ‹è¯• Topics
   â”œâ”€â”€ åœæ­¢ç³»ç»Ÿç›‘æ§
   â””â”€â”€ ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
```

#### 1.2.4 é«˜æ€§èƒ½ä¼˜åŒ–è®¾è®¡

**ä¼˜åŒ–ç­–ç•¥ 1: å¤šè¿›ç¨‹å¹¶è¡Œå‘é€** (`parallel_sender.py`):
```python
# åœºæ™¯: Producer é€Ÿç‡ > 1500 msg/s
ParallelSender
â”œâ”€â”€ åˆ›å»º N ä¸ªç‹¬ç«‹è¿›ç¨‹ (é»˜è®¤ 4)
â”œâ”€â”€ æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹çš„ Kafka Producer
â”œâ”€â”€ æ¶ˆæ¯å‡åŒ€åˆ†é…åˆ°å„è¿›ç¨‹
â”œâ”€â”€ ä½¿ç”¨ multiprocessing.Queue é€šä¿¡
â””â”€â”€ æ”¶é›†å»¶è¿Ÿç»Ÿè®¡åˆ°å…±äº«å†…å­˜

ä¼˜ç‚¹:
- ç»•è¿‡ Python GIL é™åˆ¶
- åˆ©ç”¨å¤šæ ¸ CPU
- é€‚åˆé«˜åååœºæ™¯

ç¼ºç‚¹:
- è¿›ç¨‹åˆ›å»ºå¼€é”€ (500ms+)
- è¿›ç¨‹é—´é€šä¿¡å»¶è¿Ÿ
- å†…å­˜å ç”¨å¢åŠ 
```

**ä¼˜åŒ–ç­–ç•¥ 2: è¿æ¥æ± ç®¡ç†** (`kafka_worker.py:503-602`):
```python
# é¢„åˆ›å»ºè¿æ¥ï¼Œå‡å°‘å»¶è¿Ÿ
_producer_pool: List[Producer] = []  # æœ€å¤§ 10 ä¸ª
_consumer_pool: List[Consumer] = []

åˆå§‹åŒ–:
â”œâ”€â”€ é¢„åˆ›å»º 10 ä¸ª Producer è¿æ¥
â”œâ”€â”€ æ¯ä¸ªè¿æ¥é¢„åˆå§‹åŒ– (_initialize_producer)
â””â”€â”€ ç»´æŠ¤å¥åº·æ£€æŸ¥ (60 ç§’å‘¨æœŸ)

ä½¿ç”¨:
â”œâ”€â”€ ä»»åŠ¡å¼€å§‹æ—¶ä»æ± ä¸­è·å–
â”œâ”€â”€ å¥åº·æ£€æŸ¥ç¡®ä¿è¿æ¥å¯ç”¨
â””â”€â”€ ä»»åŠ¡ç»“æŸæ—¶å½’è¿˜ (æˆ–å…³é—­)

é—®é¢˜:
- å®é™…ä»£ç ä¸­ä»»åŠ¡ç»“æŸæ—¶å…³é—­è¿æ¥ (æœªå½’è¿˜)
- è¿æ¥æ± æœªçœŸæ­£å¤ç”¨
```

**ä¼˜åŒ–ç­–ç•¥ 3: æ‰¹é‡å‘é€** (`kafka_worker.py:243-279`):
```python
# åœºæ™¯: æ— é€Ÿç‡é™åˆ¶
batch_size = min(1000, num_messages // 10)

for i in range(num_messages):
    messages_to_send.append(message)

    if len(messages_to_send) >= batch_size:
        await producer.send_batch(topic, messages_to_send)
        messages_to_send.clear()

ä¼˜ç‚¹:
- å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°
- æé«˜ååé‡
```

**ä¼˜åŒ–ç­–ç•¥ 4: é€Ÿç‡æ§åˆ¶** (`rate_limiter.py`):
```python
# Token Bucket ç®—æ³•
class AsyncRateLimiter:
    def __init__(self, rate):
        self.interval = 1.0 / rate

    async def acquire(self):
        time_since_last = now - self.last_token_time
        if time_since_last < self.interval:
            await asyncio.sleep(self.interval - time_since_last)

ä½¿ç”¨åœºæ™¯:
- ä¸­ä½é€Ÿç‡ (< 1500 msg/s)
- ç²¾ç¡®æ§åˆ¶å‘é€é€Ÿç‡
- åŒ¹é…åŸç‰ˆ OMB çš„å•æ¶ˆæ¯æ—¶åº
```

#### 1.2.5 æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ

**å»¶è¿Ÿæµ‹é‡** (`latency_recorder.py`):
```python
# Producer å»¶è¿Ÿ: å‘é€æ—¶é—´ â†’ ç¡®è®¤æ—¶é—´
HdrHistogram
â”œâ”€â”€ é«˜ç²¾åº¦å»¶è¿Ÿè®°å½• (å¾®ç§’çº§)
â”œâ”€â”€ è‡ªåŠ¨è°ƒæ•´èŒƒå›´ (1us - 3600s)
â”œâ”€â”€ ä½å†…å­˜å ç”¨ (å›ºå®šå¤§å°)
â””â”€â”€ ç²¾ç¡®è®¡ç®—åˆ†ä½æ•° (p50/p95/p99/p99.9)

# Consumer E2E å»¶è¿Ÿ: å‘é€æ—¶é—´ â†’ æ¥æ”¶æ—¶é—´
EndToEndLatencyRecorder
â”œâ”€â”€ ä»æ¶ˆæ¯å¤´æå– send_timestamp
â”œâ”€â”€ è®¡ç®—ç«¯åˆ°ç«¯å»¶è¿Ÿ
â””â”€â”€ ä½¿ç”¨ HdrHistogram è®°å½•

æ¶ˆæ¯å¤´ç»“æ„:
headers = {
    'send_timestamp': str(int(time.time() * 1000)).encode(),
    'dt_sensor_id': b'sensor_001',
    'dt_batch_id': task_id.encode()
}
```

**ååé‡æµ‹é‡**:
```python
ThroughputStats:
â”œâ”€â”€ total_messages: int        # æ€»æ¶ˆæ¯æ•°
â”œâ”€â”€ total_bytes: int           # æ€»å­—èŠ‚æ•°
â”œâ”€â”€ duration_seconds: float    # æŒç»­æ—¶é—´
â”œâ”€â”€ messages_per_second: float # msg/s
â”œâ”€â”€ bytes_per_second: float    # bytes/s
â””â”€â”€ mb_per_second: float       # MB/s

è®¡ç®—å…¬å¼:
messages_per_second = total_messages / duration_seconds
mb_per_second = (total_bytes / 1024 / 1024) / duration_seconds
```

**é”™è¯¯ç»Ÿè®¡**:
```python
ErrorStats:
â”œâ”€â”€ total_errors: int                # æ€»é”™è¯¯æ•°
â”œâ”€â”€ error_rate: float                # é”™è¯¯ç‡
â””â”€â”€ error_types: Dict[str, int]      # é”™è¯¯ç±»å‹åˆ†å¸ƒ

é”™è¯¯åˆ†ç±»:
- ç½‘ç»œé”™è¯¯ (NetworkError)
- è¶…æ—¶é”™è¯¯ (TimeoutError)
- åºåˆ—åŒ–é”™è¯¯ (SerializationError)
- Kafka é”™è¯¯ (KafkaException)
```

**ç³»ç»Ÿç›‘æ§** (`monitoring.py`):
```python
SystemMonitor (ä½¿ç”¨ psutil):
â”œâ”€â”€ CPU: ä½¿ç”¨ç‡ã€æ ¸å¿ƒæ•°
â”œâ”€â”€ å†…å­˜: ä½¿ç”¨é‡ã€ä½¿ç”¨ç‡
â”œâ”€â”€ ç½‘ç»œ: å‘é€/æ¥æ”¶å­—èŠ‚æ•°
â””â”€â”€ ç£ç›˜: è¯»å†™å­—èŠ‚æ•°

é‡‡é›†å‘¨æœŸ: 1 ç§’ (å¯é…ç½®)
ç»Ÿè®¡æŒ‡æ ‡: min/max/avg/p95
```

#### 1.2.6 é…ç½®ç®¡ç†ç³»ç»Ÿ

**é…ç½®å±‚æ¬¡ç»“æ„**:
```yaml
1. BenchmarkConfig (å…¨å±€é…ç½®)
   â”œâ”€â”€ workers: [URLs]           # Worker åœ°å€åˆ—è¡¨
   â”œâ”€â”€ log_level: INFO           # æ—¥å¿—çº§åˆ«
   â”œâ”€â”€ results_dir: results/     # ç»“æœç›®å½•
   â”œâ”€â”€ enable_monitoring: true   # ç›‘æ§å¼€å…³
   â””â”€â”€ warmup_enabled: true      # é¢„çƒ­å¼€å…³

2. WorkloadConfig (å·¥ä½œè´Ÿè½½)
   â”œâ”€â”€ topics: 10                # Topic æ•°é‡
   â”œâ”€â”€ partitionsPerTopic: 16    # æ¯ä¸ª Topic çš„åˆ†åŒºæ•°
   â”œâ”€â”€ messageSize: 1024         # æ¶ˆæ¯å¤§å° (å­—èŠ‚)
   â”œâ”€â”€ producersPerTopic: 10     # æ¯ä¸ª Topic çš„ Producer æ•°
   â”œâ”€â”€ producerRate: 1000        # æ¯ç§’æ¶ˆæ¯æ•°
   â”œâ”€â”€ subscriptionsPerTopic: 2  # æ¯ä¸ª Topic çš„è®¢é˜…æ•°
   â”œâ”€â”€ consumerPerSubscription: 4 # æ¯ä¸ªè®¢é˜…çš„ Consumer æ•°
   â”œâ”€â”€ testDurationMinutes: 10   # æµ‹è¯•æŒç»­æ—¶é—´
   â””â”€â”€ warmupDurationMinutes: 2  # é¢„çƒ­æŒç»­æ—¶é—´

3. DriverConfig (é©±åŠ¨é…ç½®)
   â”œâ”€â”€ name: Kafka
   â”œâ”€â”€ driverClass: benchmark.drivers.kafka.KafkaDriver
   â”œâ”€â”€ replicationFactor: 3
   â”œâ”€â”€ commonConfig:             # é€šç”¨é…ç½®
   â”‚   â””â”€â”€ bootstrap.servers=localhost:9092
   â”œâ”€â”€ producerConfig:           # Producer é…ç½®
   â”‚   â”œâ”€â”€ acks=all
   â”‚   â”œâ”€â”€ batch.size=65536
   â”‚   â””â”€â”€ linger.ms=5
   â”œâ”€â”€ consumerConfig:           # Consumer é…ç½®
   â”‚   â”œâ”€â”€ auto.offset.reset=earliest
   â”‚   â””â”€â”€ fetch.min.bytes=1
   â””â”€â”€ topicConfig:              # Topic é…ç½®
       â”œâ”€â”€ min.insync.replicas=2
       â””â”€â”€ compression.type=lz4
```

**é…ç½®éªŒè¯** (`config_validator.py`):
```python
éªŒè¯è§„åˆ™:
1. Driver é…ç½®éªŒè¯
   â”œâ”€â”€ bootstrap.servers å¿…é¡»è®¾ç½®
   â”œâ”€â”€ acks å¿…é¡»æ˜¯ 0/1/all
   â”œâ”€â”€ batch.size å¿…é¡» > 0
   â””â”€â”€ æ£€æµ‹å±é™©é…ç½® (acks=0 è­¦å‘Š)

2. Workload é…ç½®éªŒè¯
   â”œâ”€â”€ topics >= 1
   â”œâ”€â”€ partitionsPerTopic >= 1
   â”œâ”€â”€ messageSize >= 1
   â”œâ”€â”€ producerRate >= 0
   â””â”€â”€ testDurationMinutes >= 1

3. å…¼å®¹æ€§æ£€æŸ¥
   â”œâ”€â”€ camelCase â†” snake_case è‡ªåŠ¨è½¬æ¢
   â””â”€â”€ æ”¯æŒ Java OMB é…ç½®æ–‡ä»¶
```

#### 1.2.7 ç»“æœè¾“å‡ºç³»ç»Ÿ

**ç»“æœæ ¼å¼**:
```python
BenchmarkResult:
â”œâ”€â”€ test_id: str                      # å”¯ä¸€æµ‹è¯• ID
â”œâ”€â”€ test_name: str                    # æµ‹è¯•åç§°
â”œâ”€â”€ start_time/end_time: float        # æ—¶é—´æˆ³
â”œâ”€â”€ workload_config: Dict             # å·¥ä½œè´Ÿè½½é…ç½®å¿«ç…§
â”œâ”€â”€ driver_config: Dict               # é©±åŠ¨é…ç½®å¿«ç…§
â”œâ”€â”€ producer_results: List[WorkerResult]  # æ¯ä¸ª Producer ä»»åŠ¡ç»“æœ
â”œâ”€â”€ consumer_results: List[WorkerResult]  # æ¯ä¸ª Consumer ä»»åŠ¡ç»“æœ
â”œâ”€â”€ producer_stats: ThroughputStats   # èšåˆåçš„ Producer ç»Ÿè®¡
â”œâ”€â”€ consumer_stats: ThroughputStats   # èšåˆåçš„ Consumer ç»Ÿè®¡
â”œâ”€â”€ latency_stats: LatencyStats       # èšåˆåçš„å»¶è¿Ÿç»Ÿè®¡
â””â”€â”€ system_stats: SystemStats         # ç³»ç»Ÿç›‘æ§æ•°æ®

WorkerResult:
â”œâ”€â”€ worker_id: str
â”œâ”€â”€ task_type: producer/consumer
â”œâ”€â”€ throughput: ThroughputStats
â”œâ”€â”€ latency: LatencyStats
â””â”€â”€ errors: ErrorStats
```

**è¾“å‡ºæ ¼å¼æ”¯æŒ**:
- **JSON**: å®Œæ•´è¯¦ç»†æ•°æ® (é»˜è®¤)
- **CSV**: æ‘˜è¦æ•°æ®ï¼Œä¾¿äº Excel åˆ†æ
- **Markdown**: æ¯”è¾ƒæŠ¥å‘Š
- **å¯æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰æ ¼å¼ (Plotly å›¾è¡¨ç­‰)

### 1.3 å®ç°äº®ç‚¹

#### 1.3.1 å¹‚ç­‰æ€§æµ‹è¯•è®¾è®¡
```python
# æ¯æ¬¡æµ‹è¯•åˆ›å»ºå”¯ä¸€ Topic
test_id = f"{test_name}_{int(time.time())}"
topic_names = [f"benchmark-{test_id}-topic-{i}" for i in range(topics)]

ä¼˜ç‚¹:
- é¿å…å¤šæ¬¡æµ‹è¯•ç›¸äº’å¹²æ‰°
- é¿å…å†å²æ•°æ®å½±å“ç»“æœ
- æ”¯æŒå¹¶å‘æµ‹è¯•

æ¸…ç†æœºåˆ¶:
- æµ‹è¯•ç»“æŸè‡ªåŠ¨åˆ é™¤ Topics
- å¤±è´¥æ—¶ä¹Ÿå°è¯•æ¸…ç† (ä¸é˜»å¡æµ‹è¯•)
```

#### 1.3.2 Digital Twin åœºæ™¯ä¼˜åŒ–
```python
# é’ˆå¯¹ IoT ä¼ æ„Ÿå™¨æ•°æ®æµä¼˜åŒ–
æ¶ˆæ¯å¤´è®¾è®¡:
â”œâ”€â”€ dt_sensor_id: ä¼ æ„Ÿå™¨ ID
â”œâ”€â”€ dt_timestamp: ä¼ æ„Ÿå™¨æ—¶é—´æˆ³
â”œâ”€â”€ dt_batch_id: æ‰¹æ¬¡ ID
â””â”€â”€ send_timestamp: å‘é€æ—¶é—´ (ç”¨äºå»¶è¿Ÿæµ‹é‡)

Kafka é…ç½®ä¼˜åŒ–:
â”œâ”€â”€ compression.type=lz4     # å¿«é€Ÿå‹ç¼©
â”œâ”€â”€ batch.size=65536         # å¤§æ‰¹æ¬¡
â”œâ”€â”€ linger.ms=5              # çŸ­å»¶è¿Ÿ
â””â”€â”€ max.in.flight.requests=5 # é«˜åå

åœºæ™¯æ”¯æŒ:
- 10K ä¼ æ„Ÿå™¨ @ 1Hz = 10K msg/s
- 512 å­—èŠ‚ä¼ æ„Ÿå™¨æ•°æ® (å«å…ƒæ•°æ®)
- å®æ—¶å¤„ç† (æ— ç§¯å‹)
```

#### 1.3.3 å¼‚æ­¥å¹¶å‘è®¾è®¡
```python
# å…¨é¢ä½¿ç”¨ Python asyncio
1. Coordinator å¼‚æ­¥
   â”œâ”€â”€ å¹¶å‘æ£€æŸ¥ Worker å¥åº·: asyncio.gather()
   â”œâ”€â”€ å¹¶å‘å¯åŠ¨ä»»åŠ¡: asyncio.create_task()
   â””â”€â”€ éé˜»å¡ HTTP é€šä¿¡: aiohttp

2. Worker å¼‚æ­¥
   â”œâ”€â”€ å¹¶å‘æ‰§è¡Œå¤šä¸ªä»»åŠ¡: asyncio.gather()
   â”œâ”€â”€ å¼‚æ­¥æ¶ˆæ¯å‘é€: await producer.send_message()
   â””â”€â”€ å¼‚æ­¥æ¶ˆæ¯æ¶ˆè´¹: async for message in consumer.consume_messages()

3. FastAPI åŸç”Ÿå¼‚æ­¥
   â”œâ”€â”€ async def è·¯ç”±å¤„ç†å™¨
   â””â”€â”€ è‡ªåŠ¨å¹¶å‘ç®¡ç†
```

#### 1.3.4 çµæ´»çš„æµ‹è¯•åœºæ™¯
```
é¢„å®šä¹‰ 18 ä¸ªæµ‹è¯•åœºæ™¯:
â”œâ”€â”€ å»¶è¿Ÿæµ‹è¯• (latency-test-100b.yaml)
â”œâ”€â”€ ååé‡æµ‹è¯• (throughput-test-max.yaml)
â”œâ”€â”€ æ‰©å±•æ€§æµ‹è¯• (producer-scaling-test.yaml)
â”œâ”€â”€ æ¶ˆæ¯å¤§å°æµ‹è¯• (msg-size-comparison.yaml)
â”œâ”€â”€ Digital Twin æµ‹è¯• (digital-twin-sensor-stream.yaml)
â””â”€â”€ å¿«é€Ÿæµ‹è¯• (quick-test.yaml)

è‡ªå®šä¹‰æ”¯æŒ:
- YAML æ ¼å¼æ˜“äºç¼–å†™
- æ”¯æŒ payload æ–‡ä»¶
- æ”¯æŒä¸åŒå¯†é’¥åˆ†å¸ƒç­–ç•¥ (NO_KEY/RANDOM/ROUND_ROBIN/ZIP_LATENT)
```

#### 1.3.5 å®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿ
```python
LoggerMixin:
â”œâ”€â”€ ç»Ÿä¸€çš„æ—¥å¿—æ¥å£
â”œâ”€â”€ æ”¯æŒå¤šçº§åˆ« (DEBUG/INFO/WARNING/ERROR)
â”œâ”€â”€ Rich æ ¼å¼åŒ–è¾“å‡º (å½©è‰²ã€è¡¨æ ¼ã€è¿›åº¦æ¡)
â””â”€â”€ æ–‡ä»¶è¾“å‡ºæ”¯æŒ

æ—¥å¿—ç¤ºä¾‹:
ğŸš€ Starting benchmark: Digital Twin Sensor Stream Test
ğŸ¯ Test ID: digital_twin_1727654400
ğŸ“‹ Unique topics: [benchmark-digital_twin_1727654400-topic-0, ...]
ğŸ”§ Setting up topics...
âœ… Topic setup completed
ğŸ”¥ Starting warmup phase (2 minutes)...
âœ… Warmup phase completed
ğŸš€ Starting main test phase (10 minutes)...
ğŸ“¥ Starting consumer tasks on 2 workers...
ğŸ“¤ Starting producer tasks on 2 workers...
âœ… Main test phase completed
```

### 1.4 æŠ€æœ¯æ ˆ

**æ ¸å¿ƒä¾èµ–**:
```python
fastapi>=0.104.0          # Worker REST API
uvicorn[standard]>=0.24.0 # ASGI æœåŠ¡å™¨
pydantic>=2.0.0           # æ•°æ®éªŒè¯
pyyaml>=6.0               # é…ç½®è§£æ
aiohttp>=3.9.0            # å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
confluent-kafka>=2.3.0    # Kafka å®¢æˆ·ç«¯
numpy>=1.24.0             # æ•°å€¼è®¡ç®—
pandas>=2.0.0             # æ•°æ®å¤„ç†
psutil>=5.9.0             # ç³»ç»Ÿç›‘æ§
click>=8.1.0              # CLI å·¥å…·
rich>=13.0.0              # ç»ˆç«¯æ ¼å¼åŒ–
```

**Python ç‰ˆæœ¬**: >= 3.9

**å¼€å‘å·¥å…·**:
- pytest (å•å…ƒæµ‹è¯•)
- black (ä»£ç æ ¼å¼åŒ–)
- mypy (ç±»å‹æ£€æŸ¥)
- Docker (å®¹å™¨åŒ–éƒ¨ç½²)

---

## äºŒã€ä¸åŸç‰ˆ OMB å¯¹æ¯”çš„è®¾è®¡ç¼ºé™·

### 2.1 æ¶æ„å±‚é¢çš„é—®é¢˜

#### é—®é¢˜ 1: å¼‚æ­¥æ¨¡å‹æ··ä¹±

**åŸç‰ˆ OMB (Java)**:
```java
// æ¸…æ™°çš„å¼‚æ­¥æ¨¡å‹
CompletableFuture<RecordMetadata> future =
    producer.send(record);

future.whenComplete((metadata, exception) -> {
    // å›è°ƒå¤„ç†
    latencyRecorder.recordLatency(latency);
});
```

**å½“å‰ Python å®ç°**:
```python
# æ··åˆäº†ä¸‰ç§æ¨¡å‹
1. asyncio (å¼‚æ­¥ I/O)
2. multiprocessing (å¤šè¿›ç¨‹)
3. ThreadPoolExecutor (çº¿ç¨‹æ± )

# é—®é¢˜ä»£ç  (kafka_producer.py)
async def send_message(self, topic, message):
    # âŒ å°†éé˜»å¡çš„ produce åŒ…è£…æˆé˜»å¡è°ƒç”¨
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        self._executor,           # çº¿ç¨‹æ± 
        self._sync_send,          # åŒæ­¥åŒ…è£…
        topic, message
    )

def _sync_send(self, topic, message):
    # confluent-kafka çš„ produce æœ¬èº«å°±æ˜¯å¼‚æ­¥éé˜»å¡
    self._producer.produce(topic, message.value)
    self._producer.poll(0)  # è§¦å‘å›è°ƒ
```

**é—®é¢˜åˆ†æ**:
1. **è¿‡åº¦åŒ…è£…**: `confluent-kafka` çš„ `produce()` å·²ç»æ˜¯éé˜»å¡å¼‚æ­¥è°ƒç”¨ï¼Œä½¿ç”¨ `run_in_executor` åè€Œå¼•å…¥çº¿ç¨‹å¼€é”€
2. **æ€§èƒ½æŸå¤±**: æ¯æ¬¡å‘é€æ¶ˆæ¯éƒ½è¦ç»è¿‡: asyncio â†’ çº¿ç¨‹æ±  â†’ Kafka APIï¼Œå¢åŠ  100-500us å»¶è¿Ÿ
3. **èµ„æºæµªè´¹**: ç»´æŠ¤é¢å¤–çš„çº¿ç¨‹æ±  (`ThreadPoolExecutor`)

**æ­£ç¡®åšæ³•**:
```python
def send_message(self, topic: str, message: Message):
    """ç›´æ¥è°ƒç”¨éé˜»å¡çš„ produceï¼Œä¸éœ€è¦ async"""
    self._producer.produce(
        topic,
        value=message.value,
        key=message.key,
        headers=message.headers,
        on_delivery=self._delivery_callback
    )
    # poll(0) ä¸é˜»å¡ï¼Œä»…è§¦å‘å›è°ƒ
    self._producer.poll(0)
```

#### é—®é¢˜ 2: è¿æ¥æ± è¿‡åº¦è®¾è®¡

**åŸç‰ˆ OMB (Java)**:
```java
// æ¯ä¸ª WorkerTask ç‹¬ç«‹çš„ Producer/Consumer
class ProducerWorker implements Callable<ProducerResult> {
    private final KafkaProducer<String, byte[]> producer;

    public ProducerWorker() {
        // ä»»åŠ¡åˆ›å»ºæ—¶åˆ›å»ºè¿æ¥
        this.producer = new KafkaProducer<>(config);
    }

    public ProducerResult call() {
        // ä½¿ç”¨è¿æ¥å‘é€æ¶ˆæ¯
        // ...
        producer.close();  // ä»»åŠ¡ç»“æŸæ—¶å…³é—­
    }
}
```

**å½“å‰ Python å®ç°**:
```python
# kafka_worker.py:503-602
class KafkaWorker:
    def __init__(self):
        self._producer_pool: List[Producer] = []
        self._max_pool_size = 10

    async def _initialize_pools(self):
        """é¢„åˆ›å»º 10 ä¸ª Producer è¿æ¥"""
        for i in range(self._max_pool_size):
            producer = self._driver.create_producer()
            await producer._initialize_producer()  # âŒ è®¿é—®ç§æœ‰æ–¹æ³•
            self._producer_pool.append(producer)

    async def _execute_producer_task(self, task):
        # ä»æ± ä¸­è·å–
        producer = await self._get_producer_from_pool()
        try:
            # æ‰§è¡Œä»»åŠ¡...
        finally:
            # âŒ å¹¶æ²¡æœ‰å½’è¿˜ï¼Œè€Œæ˜¯ç›´æ¥å…³é—­
            await producer.close()  # Line 320
```

**é—®é¢˜åˆ†æ**:
1. **æ± æœªç”Ÿæ•ˆ**: é¢„åˆ›å»ºçš„ 10 ä¸ªè¿æ¥ä»æœªè¢«çœŸæ­£å¤ç”¨ï¼Œæ¯æ¬¡ä»»åŠ¡ç»“æŸéƒ½å…³é—­è¿æ¥
2. **å°è£…ç ´å**: è°ƒç”¨ `_initialize_producer()` ç§æœ‰æ–¹æ³•ï¼Œè¿åå°è£…åŸåˆ™
3. **å¤æ‚åº¦å¢åŠ **: è¿æ¥æ± å¥åº·æ£€æŸ¥ã€è·å–/å½’è¿˜é€»è¾‘å¤æ‚ï¼Œä½†å®Œå…¨æ²¡æœ‰ä»·å€¼
4. **å†…å­˜æµªè´¹**: é¢„åˆ›å»ºçš„è¿æ¥å ç”¨å†…å­˜ï¼Œä½†ä»æœªä½¿ç”¨

**æ­£ç¡®åšæ³•**:
```python
async def _execute_producer_task(self, task):
    """æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹åˆ›å»ºè¿æ¥ï¼Œç®€å•å¯é """
    producer = self._driver.create_producer()
    try:
        # æ‰§è¡Œä»»åŠ¡
        for i in range(task.num_messages):
            await producer.send_message(task.topic, message)
        await producer.flush()
    finally:
        await producer.close()
```

#### é—®é¢˜ 3: å¤šè¿›ç¨‹è®¾è®¡çš„é”™è¯¯å‡è®¾

**è®¾è®¡å‡è®¾**:
```python
# parallel_sender.py
# å‡è®¾: Python GIL é™åˆ¶å•è¿›ç¨‹ååé‡ï¼Œéœ€è¦å¤šè¿›ç¨‹ç»•è¿‡

class ParallelSender:
    def __init__(self, num_processes=4):
        self.num_processes = num_processes

    async def send_parallel(self, ...):
        # åˆ›å»º 4 ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹å‘é€
        processes = []
        for i in range(self.num_processes):
            p = Process(target=self._sender_process, ...)
            p.start()
            processes.append(p)
```

**å®é™…é—®é¢˜**:
1. **é”™è¯¯å‡è®¾**: `confluent-kafka` æ˜¯ C æ‰©å±•ï¼Œæœ¬èº«å°±ç»•è¿‡äº† GILï¼Œä¸éœ€è¦å¤šè¿›ç¨‹
2. **æ€§èƒ½ä¸‹é™**: å®æµ‹æ•°æ®
   ```
   å•è¿›ç¨‹å¼‚æ­¥: 60,000 msg/s
   4 è¿›ç¨‹å¹¶è¡Œ: 55,000 msg/s  (åè€Œé™ä½ 8%)
   ```
3. **å¼€é”€åˆ†æ**:
   - è¿›ç¨‹åˆ›å»º: 500-800ms
   - è¿›ç¨‹é—´é€šä¿¡ (Queue): æ¯æ¡æ¶ˆæ¯ 10-50us
   - å†…å­˜å ç”¨: 4x (æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹çš„ Kafka ç¼“å†²åŒº)

**åŸç‰ˆ OMB çš„åšæ³•**:
```java
// å•çº¿ç¨‹ + å¼‚æ­¥å›è°ƒå³å¯è¾¾åˆ°é«˜åå
for (int i = 0; i < numMessages; i++) {
    producer.send(record, (metadata, exception) -> {
        // å›è°ƒåœ¨ I/O çº¿ç¨‹æ‰§è¡Œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
    });
}
```

### 2.2 æ—¶åºæ§åˆ¶é—®é¢˜

#### é—®é¢˜ 4: Consumer å¯åŠ¨åŒæ­¥æœºåˆ¶ç¼ºå¤±

**åŸç‰ˆ OMB (Java)**:
```java
// WorkersEnsemble.java
public class WorkersEnsemble {
    // ä½¿ç”¨ CountdownLatch ç²¾ç¡®åŒæ­¥
    private final CountDownLatch consumersLatch;

    public void startConsumers() {
        consumersLatch = new CountDownLatch(totalConsumers);

        for (ConsumerWorker worker : consumerWorkers) {
            worker.setReadyCallback(() -> {
                consumersLatch.countDown();  // Consumer å°±ç»ªåè®¡æ•°
            });
            worker.start();
        }

        // ç­‰å¾…æ‰€æœ‰ Consumer ç¡®è®¤è®¢é˜…
        consumersLatch.await(60, TimeUnit.SECONDS);

        if (consumersLatch.getCount() > 0) {
            throw new TimeoutException("Some consumers not ready");
        }
    }
}
```

**å½“å‰ Python å®ç°**:
```python
# coordinator.py:236-253
async def _run_test_phase(self, ...):
    # å¯åŠ¨ Consumer ä»»åŠ¡
    consumer_futures = []
    for worker_url in self.config.workers:
        future = asyncio.create_task(
            self._run_worker_consumer_tasks(worker_url, tasks)
        )
        consumer_futures.append(future)

    # âŒ ç¡¬ç¼–ç ç­‰å¾… 5 ç§’
    self.logger.info("â±ï¸  Waiting for consumers to subscribe...")
    await asyncio.sleep(5)  # Line 252
    self.logger.info("âœ… Consumers should now be ready")

    # å¯åŠ¨ Producer ä»»åŠ¡
    # ...
```

**é—®é¢˜åˆ†æ**:
1. **ä¸å¯é **: 5 ç§’æ˜¯ç»éªŒå€¼ï¼Œæ— æ³•ä¿è¯ Consumer çœŸæ­£å°±ç»ª
   - ç½‘ç»œå»¶è¿Ÿè¾ƒé«˜æ—¶å¯èƒ½ä¸å¤Ÿ
   - Kafka åˆ†åŒºåˆ†é…å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
   - å¤šä¸ª Consumer Group æ—¶å»¶è¿Ÿæ›´é•¿
2. **æ— åé¦ˆ**: ä¸çŸ¥é“ Consumer æ˜¯å¦æˆåŠŸè®¢é˜…ï¼Œå¯èƒ½å·²ç»å¤±è´¥
3. **æ€§èƒ½æµªè´¹**: Consumer å¯èƒ½ 2 ç§’å°±ç»ªï¼Œä½†ä»ç­‰å¾… 5 ç§’

**ä¼šå¯¼è‡´çš„é—®é¢˜**:
```
æµ‹è¯•åœºæ™¯: 10 topics Ã— 16 partitions, 80 consumers
æœŸæœ›è¡Œä¸º: Consumer è®¢é˜…å®Œæˆåå†å‘é€æ¶ˆæ¯

å®é™…æƒ…å†µ:
â”œâ”€â”€ t=0s:  Consumer å¼€å§‹è®¢é˜…
â”œâ”€â”€ t=2s:  50% Consumer å®Œæˆåˆ†åŒºåˆ†é…
â”œâ”€â”€ t=4s:  80% Consumer å®Œæˆåˆ†åŒºåˆ†é…
â”œâ”€â”€ t=5s:  Producer å¼€å§‹å‘é€ (20% Consumer æœªå°±ç»ª)
â”œâ”€â”€ t=7s:  100% Consumer å®Œæˆåˆ†åŒºåˆ†é…
â””â”€â”€ ç»“æœ: å‰ 2 ç§’çš„æ¶ˆæ¯éƒ¨åˆ†ä¸¢å¤±ï¼Œååé‡æµ‹ä¸å‡†
```

**æ­£ç¡®åšæ³•**:
```python
# Worker API å¢åŠ è®¢é˜…ç¡®è®¤ç«¯ç‚¹
@app.get("/consumer/{task_id}/subscription_status")
async def get_subscription_status(task_id: str):
    consumer = get_consumer(task_id)
    return {
        "subscribed": consumer.is_subscribed(),
        "assigned_partitions": consumer.assignment(),
        "ready": len(consumer.assignment()) > 0
    }

# Coordinator è½®è¯¢ç­‰å¾…
async def _wait_for_consumers_ready(self, consumer_tasks):
    timeout = 60  # æœ€å¤šç­‰å¾… 60 ç§’
    start_time = time.time()

    while time.time() - start_time < timeout:
        all_ready = True
        for task in consumer_tasks:
            status = await self._get_consumer_status(task.task_id)
            if not status['ready']:
                all_ready = False
                break

        if all_ready:
            self.logger.info("âœ… All consumers ready")
            return

        await asyncio.sleep(0.5)

    raise TimeoutException("Consumers not ready within timeout")
```

#### é—®é¢˜ 5: Producer ç»“æœæ”¶é›†æ—¶æœºé”™è¯¯

**åŸç‰ˆ OMB (Java)**:
```java
// ProducerWorker.java
public class ProducerWorker {
    public ProducerResult call() {
        // å‘é€æ‰€æœ‰æ¶ˆæ¯
        for (int i = 0; i < numMessages; i++) {
            producer.send(record, callback);
        }

        // ç­‰å¾…æ‰€æœ‰å›è°ƒå®Œæˆ
        producer.flush();  // é˜»å¡ç›´åˆ°æ‰€æœ‰æ¶ˆæ¯ç¡®è®¤

        // æ­¤æ—¶æ‰€æœ‰å»¶è¿Ÿéƒ½å·²è®°å½•
        LatencyStats stats = latencyRecorder.getSnapshot();

        return new ProducerResult(stats);
    }
}
```

**å½“å‰ Python å®ç°**:
```python
# kafka_worker.py:281-322
async def _execute_producer_task(self, task):
    try:
        # å‘é€æ¶ˆæ¯...

        # Flush ç¼“å†²åŒº
        await producer.flush()  # Line 282

        # âŒ ç­‰å¾… deliveryï¼Œä½†æœ‰è¶…æ—¶
        delivery_status = await producer.wait_for_delivery(timeout_seconds=60)
        if delivery_status.get('timed_out', False):
            # âš ï¸ è¶…æ—¶äº†ï¼Œä½†ç»§ç»­æ‰§è¡Œ
            self.logger.warning(f"Delivery timeout: {delivery_status}")

    finally:
        # è·å–å»¶è¿Ÿç»Ÿè®¡
        latency_snapshot = producer.get_latency_snapshot()  # Line 306
        messages_sent = latency_snapshot.count

        # âŒ å¯èƒ½ä¸åŒ¹é…
        if messages_sent != task.num_messages:
            self.logger.warning(
                f"Message count mismatch: expected {task.num_messages}, "
                f"got {messages_sent}"  # Line 310-314
            )

        # å…³é—­ Producer
        await producer.close()  # Line 320
```

**é—®é¢˜åˆ†æ**:
1. **ç»Ÿè®¡ä¸å®Œæ•´**: `flush()` åç«‹å³è·å–ç»Ÿè®¡ï¼Œå¯èƒ½éƒ¨åˆ†å›è°ƒè¿˜æœªæ‰§è¡Œ
   ```python
   Timeline:
   t=0:   å‘é€ 1000 æ¡æ¶ˆæ¯
   t=1:   flush() å®Œæˆ (ç¼“å†²åŒºæ¸…ç©º)
   t=1.1: è·å– latency_snapshot (å¯èƒ½åªæœ‰ 950 æ¡)
   t=1.5: å‰©ä½™ 50 æ¡å›è°ƒæ‰§è¡Œå®Œæ¯•
   ```
2. **è¶…æ—¶å¤„ç†ä¸å½“**: `wait_for_delivery()` è¶…æ—¶åä»…è®°å½•è­¦å‘Šï¼Œä½†ç»Ÿè®¡æ•°æ®å·²ä¸å‡†ç¡®
3. **æ¶ˆæ¯æ•°ä¸åŒ¹é…**: é¢‘ç¹å‡ºç° `messages_sent != num_messages`ï¼Œè¯´æ˜å›è°ƒæœªå…¨éƒ¨æ‰§è¡Œ

**ä¼šå¯¼è‡´çš„é—®é¢˜**:
```python
æµ‹è¯•é…ç½®: å‘é€ 100,000 æ¡æ¶ˆæ¯
å®é™…ç»“æœ: latency_snapshot.count = 99,850

å»¶è¿Ÿç»Ÿè®¡ç¼ºå¤± 150 æ¡æ ·æœ¬:
- å¯èƒ½æ˜¯æœ€æ…¢çš„ 150 æ¡ (å½±å“ p99.9)
- ä¹Ÿå¯èƒ½æ˜¯éšæœºçš„ 150 æ¡
- å¯¼è‡´å»¶è¿Ÿåˆ†ä½æ•°ä¸å‡†ç¡®
```

**æ­£ç¡®åšæ³•**:
```python
async def _execute_producer_task(self, task):
    # åˆ›å»ºå€’è®¡æ—¶å™¨
    pending_count = task.num_messages
    pending_latch = asyncio.Event()

    def on_delivery(err, msg):
        nonlocal pending_count
        if err is None:
            latency = calculate_latency(msg)
            latency_recorder.record(latency)
        pending_count -= 1
        if pending_count == 0:
            pending_latch.set()  # å…¨éƒ¨å®Œæˆ

    # å‘é€æ¶ˆæ¯
    for i in range(task.num_messages):
        producer.produce(topic, message, callback=on_delivery)

    # ç­‰å¾…æ‰€æœ‰å›è°ƒå®Œæˆ (æœ‰è¶…æ—¶)
    try:
        await asyncio.wait_for(pending_latch.wait(), timeout=120)
    except asyncio.TimeoutError:
        raise Exception(f"Delivery timeout: {pending_count} messages pending")

    # æ­¤æ—¶ç»Ÿè®¡æ˜¯å®Œæ•´çš„
    return latency_recorder.get_snapshot()
```

### 2.3 ç»Ÿè®¡èšåˆé—®é¢˜

#### é—®é¢˜ 6: å»¶è¿Ÿåˆ†ä½æ•°èšåˆé”™è¯¯

**åŸç‰ˆ OMB (Java)**:
```java
// AggregatedResult.java
public class AggregatedResult {
    public static LatencyStats aggregate(List<LatencyStats> statsList) {
        // ä½¿ç”¨ HdrHistogram åˆå¹¶æ‰€æœ‰æ ·æœ¬
        Histogram mergedHistogram = new Histogram(3600000000000L, 3);

        for (LatencyStats stats : statsList) {
            mergedHistogram.add(stats.getHistogram());
        }

        // ä»åˆå¹¶åçš„ç›´æ–¹å›¾è®¡ç®—åˆ†ä½æ•°
        return new LatencyStats(
            mergedHistogram.getValueAtPercentile(50.0),   // p50
            mergedHistogram.getValueAtPercentile(95.0),   // p95
            mergedHistogram.getValueAtPercentile(99.0),   // p99
            mergedHistogram.getValueAtPercentile(99.9)    // p99.9
        );
    }
}
```

**å½“å‰ Python å®ç°**:
```python
# results.py:207-231
def _aggregate_latency_stats(self, stats_list: List[LatencyStats]):
    """èšåˆå»¶è¿Ÿç»Ÿè®¡"""
    if not stats_list:
        return LatencyStats()

    total_count = sum(s.count for s in stats_list)

    # âœ… åŠ æƒå¹³å‡ (æ­£ç¡®)
    weighted_mean = sum(s.mean_ms * s.count for s in stats_list) / total_count

    return LatencyStats(
        count=total_count,
        min_ms=min(s.min_ms for s in stats_list),  # âœ… æ­£ç¡®
        max_ms=max(s.max_ms for s in stats_list),  # âœ… æ­£ç¡®
        mean_ms=weighted_mean,                     # âœ… æ­£ç¡®

        # âŒ ä»¥ä¸‹å…¨éƒ¨é”™è¯¯
        median_ms=np.mean([s.median_ms for s in stats_list]),  # Line 226
        p50_ms=np.mean([s.p50_ms for s in stats_list]),        # Line 227
        p95_ms=max(s.p95_ms for s in stats_list),              # Line 228
        p99_ms=max(s.p99_ms for s in stats_list),              # Line 229
        p99_9_ms=max(s.p99_9_ms for s in stats_list)           # Line 230
    )
```

**é—®é¢˜åˆ†æ**:

**åœºæ™¯ 1: p50 å–å¹³å‡å€¼é”™è¯¯**
```python
Worker 1: 1000 æ¡æ ·æœ¬, p50 = 10ms
Worker 2: 100 æ¡æ ·æœ¬,  p50 = 50ms

å½“å‰åšæ³• (å¹³å‡):
aggregated_p50 = (10 + 50) / 2 = 30ms  # âŒ é”™è¯¯

æ­£ç¡®åšæ³• (åˆå¹¶ç›´æ–¹å›¾):
æ€»å…± 1100 æ¡æ ·æœ¬çš„ p50 = 12ms  # âœ… æ­£ç¡®
(å› ä¸º Worker 1 è´¡çŒ®äº† 90% çš„æ ·æœ¬)
```

**åœºæ™¯ 2: p99 å–æœ€å¤§å€¼é”™è¯¯**
```python
Worker 1: 10,000 æ¡æ ·æœ¬, p99 = 20ms (100 æ¡ > 20ms)
Worker 2: 10,000 æ¡æ ·æœ¬, p99 = 100ms (100 æ¡ > 100ms)

å½“å‰åšæ³• (æœ€å¤§å€¼):
aggregated_p99 = max(20, 100) = 100ms  # âŒ ä¸¥é‡åé«˜

æ­£ç¡®åšæ³• (åˆå¹¶ç›´æ–¹å›¾):
æ€»å…± 20,000 æ¡æ ·æœ¬çš„ p99 = 50ms  # âœ… æ­£ç¡®
(å› ä¸ºåªæœ‰ 1% çš„æ ·æœ¬ > 50ms, å³ 200 æ¡)
```

**çœŸå®å½±å“**:
```python
æµ‹è¯•: 4 ä¸ª Worker, æ¯ä¸ª 25,000 æ¡æ¶ˆæ¯

å®é™…å»¶è¿Ÿåˆ†å¸ƒ:
â”œâ”€â”€ Worker 1: p99 = 15ms
â”œâ”€â”€ Worker 2: p99 = 18ms
â”œâ”€â”€ Worker 3: p99 = 22ms
â””â”€â”€ Worker 4: p99 = 95ms (æœ‰ç½‘ç»œæŠ–åŠ¨)

å½“å‰èšåˆç»“æœ: p99 = 95ms  # âŒ å–æœ€å¤§å€¼
æ­£ç¡®èšåˆç»“æœ: p99 = 20ms  # âœ… åˆå¹¶ç›´æ–¹å›¾

åå·®: 375% è¯¯å·®ï¼
```

**æ­£ç¡®åšæ³•**:
```python
def _aggregate_latency_stats(self, stats_list: List[LatencyStats]):
    """æ­£ç¡®èšåˆ: åˆå¹¶ HdrHistogram"""
    from hdrh.histogram import HdrHistogram

    # åˆ›å»ºåˆå¹¶åçš„ç›´æ–¹å›¾
    merged = HdrHistogram(1, 3600000000, 3)

    for stats in stats_list:
        # éœ€è¦ä¿ç•™åŸå§‹ç›´æ–¹å›¾
        merged.add(stats.histogram)

    return LatencyStats(
        count=merged.get_total_count(),
        min_ms=merged.get_min_value() / 1000.0,
        max_ms=merged.get_max_value() / 1000.0,
        mean_ms=merged.get_mean_value() / 1000.0,
        p50_ms=merged.get_value_at_percentile(50.0) / 1000.0,
        p95_ms=merged.get_value_at_percentile(95.0) / 1000.0,
        p99_ms=merged.get_value_at_percentile(99.0) / 1000.0,
        p99_9_ms=merged.get_value_at_percentile(99.9) / 1000.0
    )
```

**éœ€è¦ä¿®æ”¹çš„æ•°æ®ç»“æ„**:
```python
@dataclass
class LatencyStats:
    count: int = 0
    min_ms: float = 0.0
    # ...
    p99_9_ms: float = 0.0

    # âœ… å¢åŠ : ä¿ç•™åŸå§‹ç›´æ–¹å›¾ç”¨äºèšåˆ
    histogram: Optional[HdrHistogram] = None
```

#### é—®é¢˜ 7: ååé‡èšåˆä¸å‡†ç¡®

**å½“å‰å®ç°**:
```python
# results.py:189-205
def _aggregate_throughput_stats(self, stats_list):
    total_messages = sum(s.total_messages for s in stats_list)  # âœ…
    total_bytes = sum(s.total_bytes for s in stats_list)        # âœ…

    # âŒ ä½¿ç”¨æœ€é•¿æŒç»­æ—¶é—´
    max_duration = max(s.duration_seconds for s in stats_list)  # Line 196

    return ThroughputStats(
        messages_per_second=total_messages / max_duration,      # âŒ
        mb_per_second=(total_bytes / 1024 / 1024) / max_duration  # âŒ
    )
```

**é—®é¢˜åœºæ™¯**:
```python
åœºæ™¯: 3 ä¸ª Worker å¹¶å‘å‘é€æ¶ˆæ¯

Worker 1: 10,000 æ¡, è€—æ—¶ 10s  â†’ 1,000 msg/s
Worker 2: 10,000 æ¡, è€—æ—¶ 10s  â†’ 1,000 msg/s
Worker 3: 10,000 æ¡, è€—æ—¶ 12s  â†’ 833 msg/s (ç½‘ç»œæ…¢)

å½“å‰èšåˆ:
total_messages = 30,000
max_duration = 12s
throughput = 30,000 / 12 = 2,500 msg/s  # âŒ

å®é™…ååé‡ (åº”è¯¥è€ƒè™‘å¹¶å‘):
- å‰ 10 ç§’: 3 ä¸ª Worker å¹¶å‘, ~3,000 msg/s
- 10-12 ç§’: ä»… Worker 3, ~833 msg/s
- å¹³å‡: (3000 * 10 + 833 * 2) / 12 = 2,638 msg/s  # âœ…
```

**æ›´ä¸¥é‡çš„é—®é¢˜**:
```python
åœºæ™¯: 4 ä¸ª Worker, å…¶ä¸­ 1 ä¸ªæå‰å®Œæˆ

Worker 1: 25,000 æ¡, t=0-10s   (å®Œæˆ)
Worker 2: 25,000 æ¡, t=2-12s   (å»¶è¿Ÿå¯åŠ¨)
Worker 3: 25,000 æ¡, t=0-10s   (å®Œæˆ)
Worker 4: 25,000 æ¡, t=0-10s   (å®Œæˆ)

å½“å‰èšåˆ:
max_duration = 12s
throughput = 100,000 / 12 = 8,333 msg/s  # âŒ

å®é™…å³°å€¼ååé‡:
t=2-10s: 4 ä¸ª Worker å¹¶å‘, 100,000 / 8 = 12,500 msg/s  # âœ…
```

**åŸç‰ˆ OMB çš„åšæ³•**:
```java
// ä½¿ç”¨å®é™…çš„å¹¶å‘æ—¶é—´çª—å£
public ThroughputStats aggregate(List<WorkerResult> results) {
    // æ‰¾åˆ°æœ€æ—©å¼€å§‹æ—¶é—´å’Œæœ€æ™šç»“æŸæ—¶é—´
    long minStartTime = results.stream()
        .mapToLong(WorkerResult::getStartTime)
        .min().orElse(0);

    long maxEndTime = results.stream()
        .mapToLong(WorkerResult::getEndTime)
        .max().orElse(0);

    double actualDuration = (maxEndTime - minStartTime) / 1000.0;
    double throughput = totalMessages / actualDuration;

    return new ThroughputStats(totalMessages, throughput);
}
```

**æ­£ç¡®åšæ³•**:
```python
def _aggregate_throughput_stats(self, stats_list):
    if not stats_list:
        return ThroughputStats()

    total_messages = sum(s.total_messages for s in stats_list)
    total_bytes = sum(s.total_bytes for s in stats_list)

    # âœ… ä½¿ç”¨å®é™…çš„å¹¶å‘æ—¶é—´çª—å£
    # éœ€è¦ä» WorkerResult è·å– start_time å’Œ end_time
    min_start_time = min(r.start_time for r in self.worker_results)
    max_end_time = max(r.end_time for r in self.worker_results)
    actual_duration = max_end_time - min_start_time

    return ThroughputStats(
        total_messages=total_messages,
        total_bytes=total_bytes,
        duration_seconds=actual_duration,
        messages_per_second=total_messages / actual_duration,
        mb_per_second=(total_bytes / 1024 / 1024) / actual_duration
    )
```

### 2.4 é…ç½®å¤„ç†é—®é¢˜

#### é—®é¢˜ 8: é…ç½®ç±»å‹è½¬æ¢ç¼ºå¤±

**å½“å‰å®ç°**:
```python
# config.py:66-78
@validator('common_config', 'producer_config', 'consumer_config', 'topic_config')
def parse_config_string(cls, v):
    if isinstance(v, str):
        config = {}
        for line in v.strip().split('\n'):
            if '=' in line and not line.startswith('#'):
                key, value = line.split('=', 1)
                config[key.strip()] = value.strip()  # âŒ å…¨éƒ¨å­—ç¬¦ä¸²
        return config
    return v or {}
```

**é—®é¢˜åœºæ™¯**:
```yaml
# kafka-digital-twin.yaml
producerConfig: |
  batch.size=65536              # åº”è¯¥æ˜¯ int
  linger.ms=5                   # åº”è¯¥æ˜¯ int
  enable.idempotence=true       # åº”è¯¥æ˜¯ bool
  acks=all                      # åº”è¯¥æ˜¯ str (ç‰¹æ®Šå€¼)
  compression.type=lz4          # åº”è¯¥æ˜¯ str

è§£æç»“æœ (å…¨éƒ¨å­—ç¬¦ä¸²):
{
    'batch.size': '65536',             # âŒ str, åº”è¯¥æ˜¯ int
    'linger.ms': '5',                  # âŒ str, åº”è¯¥æ˜¯ int
    'enable.idempotence': 'true',      # âŒ str, åº”è¯¥æ˜¯ bool
    'acks': 'all',                     # âœ… str (æ­£ç¡®)
    'compression.type': 'lz4'          # âœ… str (æ­£ç¡®)
}
```

**å®é™…å½±å“**:
```python
# confluent-kafka çš„ Producer é…ç½®å¤„ç†
producer = Producer({
    'bootstrap.servers': 'localhost:9092',
    'batch.size': '65536',  # âŒ å­—ç¬¦ä¸²
})

# librdkafka (confluent-kafka çš„ C åº•å±‚) è¡Œä¸º:
1. å°è¯•å°† '65536' è½¬æ¢ä¸º int
2. æˆåŠŸ â†’ ä½¿ç”¨ 65536
3. å¤±è´¥ â†’ ä½¿ç”¨é»˜è®¤å€¼ 16384

é—®é¢˜:
- éƒ¨åˆ†å‚æ•°å¯ä»¥æ­£ç¡®è½¬æ¢ (batch.size)
- éƒ¨åˆ†å‚æ•°è½¬æ¢å¤±è´¥ä¼šé™é»˜ä½¿ç”¨é»˜è®¤å€¼
- å¸ƒå°”å€¼ 'true' å¯èƒ½è¢«è§£æä¸º true/1/å­—ç¬¦ä¸² (ä¸ç¡®å®š)
- æ²¡æœ‰æ˜ç¡®çš„é”™è¯¯æç¤º
```

**çœŸå®æ¡ˆä¾‹**:
```python
é…ç½®: enable.idempotence=true

é¢„æœŸè¡Œä¸º: Producer å¯ç”¨å¹‚ç­‰æ€§
â”œâ”€â”€ exactly-once è¯­ä¹‰
â”œâ”€â”€ è‡ªåŠ¨å»é‡
â””â”€â”€ æ¶ˆæ¯é¡ºåºä¿è¯

å®é™…è¡Œä¸º (å¦‚æœè½¬æ¢å¤±è´¥):
â”œâ”€â”€ 'true' è¢«å½“ä½œå­—ç¬¦ä¸²
â”œâ”€â”€ librdkafka æ— æ³•è¯†åˆ«
â”œâ”€â”€ ä½¿ç”¨é»˜è®¤å€¼ false
â””â”€â”€ æ²¡æœ‰å¹‚ç­‰æ€§ä¿è¯ (å¯èƒ½é‡å¤/ä¹±åº)

æµ‹è¯•ç»“æœå·®å¼‚:
- æ— å¹‚ç­‰æ€§: æ¶ˆæ¯å¯èƒ½é‡å¤, count ä¸å‡†ç¡®
- æœ‰å¹‚ç­‰æ€§: æ¶ˆæ¯ç²¾ç¡®ä¸€æ¬¡
```

**æ­£ç¡®åšæ³•**:
```python
@validator('common_config', 'producer_config', 'consumer_config', 'topic_config')
def parse_config_string(cls, v):
    if isinstance(v, str):
        config = {}
        for line in v.strip().split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if '=' not in line:
                continue

            key, value = line.split('=', 1)
            key = key.strip()
            value = value.strip()

            # âœ… ç±»å‹è½¬æ¢
            if value.lower() in ('true', 'false'):
                config[key] = value.lower() == 'true'
            elif value.isdigit():
                config[key] = int(value)
            elif value.replace('.', '', 1).isdigit():
                config[key] = float(value)
            else:
                config[key] = value

        return config
    return v or {}
```

**æ›´å¥½çš„åšæ³• (ç±»å‹æ³¨è§£)**:
```python
# å®šä¹‰ Kafka é…ç½®æ¨¡å¼
KAFKA_CONFIG_SCHEMA = {
    'batch.size': int,
    'linger.ms': int,
    'enable.idempotence': bool,
    'acks': str,  # ç‰¹æ®Šå€¼: '0', '1', 'all'
    'compression.type': str,
    # ...
}

def parse_config_with_schema(config_str: str, schema: dict) -> dict:
    raw_config = parse_config_string(config_str)
    typed_config = {}

    for key, value in raw_config.items():
        expected_type = schema.get(key, str)  # é»˜è®¤å­—ç¬¦ä¸²
        try:
            if expected_type == bool:
                typed_config[key] = value.lower() == 'true'
            else:
                typed_config[key] = expected_type(value)
        except ValueError as e:
            raise ConfigError(f"Invalid type for {key}: {value}")

    return typed_config
```

#### é—®é¢˜ 9: Consumer é…ç½®è‡´å‘½é”™è¯¯

**é—®é¢˜é…ç½®**:
```yaml
# configs/kafka-digital-twin.yaml:59
consumerConfig: |
  auto.offset.reset=latest    # âŒ è‡´å‘½é”™è¯¯ï¼
  enable.auto.commit=false
  fetch.min.bytes=1
```

**é—®é¢˜åˆ†æ**:

**åœºæ™¯é‡ç°**:
```python
1. t=0s:  åˆ›å»º Topics
2. t=1s:  Consumer å¯åŠ¨, è®¢é˜… Topics
          â”œâ”€â”€ auto.offset.reset=latest
          â””â”€â”€ ä»æœ€æ–°ä½ç½®å¼€å§‹æ¶ˆè´¹

3. t=6s:  Producer å¼€å§‹å‘é€æ¶ˆæ¯
          â”œâ”€â”€ å‘é€ 10,000 æ¡åˆ° offset 0-9999
          â””â”€â”€ Consumer æ­¤æ—¶è¿˜æœªæ¶ˆè´¹ä»»ä½•æ¶ˆæ¯

4. t=7s:  Consumer ç¬¬ä¸€æ¬¡ poll()
          â”œâ”€â”€ offset.reset=latest ç”Ÿæ•ˆ
          â”œâ”€â”€ è·³è¿‡ offset 0-9999 (å†å²æ¶ˆæ¯)
          â””â”€â”€ ä» offset 10000 å¼€å§‹æ¶ˆè´¹

5. t=16s: Producer å‘é€å®Œæ¯•, å…± 10,000 æ¡
6. t=17s: Consumer æ¶ˆè´¹ç»Ÿè®¡: 0 æ¡ âŒ

ç»“æœ:
- Producer ååé‡: 10,000 msg / 10s = 1,000 msg/s
- Consumer ååé‡: 0 msg / 10s = 0 msg/s
- æ•°æ®å®Œå…¨ä¸åŒ¹é…ï¼
```

**å®é™…æµ‹è¯•æ—¥å¿—**:
```
ğŸš€ Starting producer tasks...
âœ… Producer task completed: 100,000 messages sent
ğŸ“Š Producer throughput: 10,000 msg/s

ğŸš€ Starting consumer tasks...
[CONSUMER DEBUG] Consumed 0 messages so far...  # âŒ
[CONSUMER DEBUG] Consumer task finished: consumed 0 messages
ğŸ“Š Consumer throughput: 0 msg/s

âŒ ERROR: Producer/Consumer ååé‡ä¸åŒ¹é…
```

**åŸç‰ˆ OMB çš„é…ç½®**:
```properties
# åŸºå‡†æµ‹è¯•å¿…é¡»ä½¿ç”¨ earliest
auto.offset.reset=earliest

# ç¡®ä¿æ¶ˆè´¹æ‰€æœ‰æµ‹è¯•æ¶ˆæ¯
- Producer å‘é€å‰çš„æ¶ˆæ¯: å¿½ç•¥ (æµ‹è¯•å‰æ¸…ç©º)
- Producer å‘é€çš„æ¶ˆæ¯: å…¨éƒ¨æ¶ˆè´¹ âœ…
- Producer å‘é€åçš„æ¶ˆæ¯: ä¸å­˜åœ¨ (æµ‹è¯•ç»“æŸ)
```

**ä¿®å¤æ–¹æ³•**:
```yaml
# configs/kafka-digital-twin.yaml
consumerConfig: |
  # âœ… åŸºå‡†æµ‹è¯•å¿…é¡»ä½¿ç”¨ earliest
  auto.offset.reset=earliest

  # âœ… æ‰‹åŠ¨æäº¤, ä¾¿äºæ§åˆ¶
  enable.auto.commit=false

  # å…¶ä»–é…ç½®ä¿æŒä¸å˜
  fetch.min.bytes=1
  max.partition.fetch.bytes=1048576
```

**éªŒè¯æ–¹æ³•**:
```python
# æ·»åŠ æ–­è¨€
async def _run_test_phase(self, ...):
    # æ‰§è¡Œæµ‹è¯•...

    # éªŒè¯æ¶ˆæ¯æ•°åŒ¹é…
    total_produced = sum(r.throughput.total_messages
                        for r in producer_results)
    total_consumed = sum(r.throughput.total_messages
                        for r in consumer_results)

    # å…è®¸ 5% è¯¯å·® (è€ƒè™‘æ—¶åº)
    if abs(total_consumed - total_produced) > total_produced * 0.05:
        raise AssertionError(
            f"Message count mismatch: "
            f"produced={total_produced}, consumed={total_consumed}"
        )
```

### 2.5 é”™è¯¯å¤„ç†ä¸å¯é æ€§é—®é¢˜

#### é—®é¢˜ 10: è¶…æ—¶æœºåˆ¶ç¼ºå¤±

**å½“å‰å®ç°**:
```python
# coordinator.py:268-299
async def _run_test_phase(self, ...):
    # å¯åŠ¨ Producer ä»»åŠ¡
    producer_futures = []
    for worker_url in self.config.workers:
        future = asyncio.create_task(
            self._run_worker_producer_tasks(worker_url, tasks)
        )
        producer_futures.append(future)

    # âŒ æ— é™ç­‰å¾…
    all_producer_results = []
    for i, future in enumerate(producer_futures):
        try:
            results = await future  # æ²¡æœ‰è¶…æ—¶é™åˆ¶
            all_producer_results.extend(results)
        except Exception as e:
            self.logger.error(f"Producer task group {i+1} failed: {e}")
            # âš ï¸ ç»§ç»­ç­‰å¾…å…¶ä»–ä»»åŠ¡
```

**é—®é¢˜åœºæ™¯**:
```python
åœºæ™¯ 1: Worker å´©æºƒ
â”œâ”€â”€ Worker 1 è¿›ç¨‹æ„å¤–é€€å‡º
â”œâ”€â”€ Coordinator ä¸€ç›´ç­‰å¾… Worker 1 çš„å“åº”
â”œâ”€â”€ æ°¸ä¹…é˜»å¡, æµ‹è¯•æ°¸ä¸ç»“æŸ
â””â”€â”€ éœ€è¦æ‰‹åŠ¨ Ctrl+C ç»ˆæ­¢

åœºæ™¯ 2: ç½‘ç»œåˆ†åŒº
â”œâ”€â”€ Worker 2 ç½‘ç»œè¿æ¥æ–­å¼€
â”œâ”€â”€ Coordinator çš„ HTTP è¯·æ±‚è¶…æ—¶ (aiohttp é»˜è®¤ 300s)
â”œâ”€â”€ 5 åˆ†é’Ÿåæ‰å¤±è´¥
â””â”€â”€ å½±å“å…¶ä»– Worker çš„ç»“æœæ”¶é›†

åœºæ™¯ 3: ä»»åŠ¡æŒ‚èµ·
â”œâ”€â”€ Producer ä»»åŠ¡å›  Kafka é—®é¢˜æŒ‚èµ·
â”œâ”€â”€ Worker API æ­£å¸¸ä½†ä»»åŠ¡ä¸è¿”å›
â”œâ”€â”€ Coordinator æ°¸ä¹…ç­‰å¾…
â””â”€â”€ æ— æ³•æ£€æµ‹å’Œæ¢å¤
```

**åŸç‰ˆ OMB (Java)**:
```java
// LocalWorker.java
public CompletableFuture<ProducerResult> runProducer(...) {
    return CompletableFuture
        .supplyAsync(() -> producerWorker.call(), executor)
        .orTimeout(testDuration + 60, TimeUnit.SECONDS)  // âœ… è¶…æ—¶
        .exceptionally(ex -> {
            log.error("Producer worker failed", ex);
            return ProducerResult.error(ex);  // âœ… è¿”å›é”™è¯¯ç»“æœ
        });
}

// Coordinator ç­‰å¾…æ—¶ä¹Ÿæœ‰è¶…æ—¶
List<ProducerResult> results = CompletableFuture
    .allOf(futures.toArray(new CompletableFuture[0]))
    .orTimeout(testDuration + 120, TimeUnit.SECONDS)  // âœ… å…¨å±€è¶…æ—¶
    .thenApply(v -> collectResults(futures))
    .get();
```

**æ­£ç¡®åšæ³•**:
```python
async def _run_test_phase(self, result, workload_config, ...):
    # è®¡ç®—è¶…æ—¶æ—¶é—´
    test_duration = workload_config.test_duration_minutes * 60
    timeout = test_duration + 120  # æµ‹è¯•æ—¶é—´ + 2 åˆ†é’Ÿç¼“å†²

    try:
        # å¯åŠ¨ä»»åŠ¡ (ä»£ç ä¸å˜)
        producer_futures = [...]
        consumer_futures = [...]

        # âœ… ä½¿ç”¨ asyncio.wait_for å¢åŠ è¶…æ—¶
        producer_results = await asyncio.wait_for(
            self._gather_producer_results(producer_futures),
            timeout=timeout
        )

        consumer_results = await asyncio.wait_for(
            self._gather_consumer_results(consumer_futures),
            timeout=timeout
        )

    except asyncio.TimeoutError:
        self.logger.error(f"Test timeout after {timeout}s")

        # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        for future in producer_futures + consumer_futures:
            if not future.done():
                future.cancel()

        # æ”¶é›†å·²å®Œæˆçš„ç»“æœ
        producer_results = self._collect_completed_results(producer_futures)
        consumer_results = self._collect_completed_results(consumer_futures)

        # æ ‡è®°æµ‹è¯•å¤±è´¥ä½†ç»§ç»­èšåˆç»“æœ
        result.metadata['timeout'] = True
        result.metadata['completed_producers'] = len(producer_results)
        result.metadata['completed_consumers'] = len(consumer_results)

async def _gather_producer_results(self, futures):
    """æ”¶é›†ç»“æœ, éƒ¨åˆ†å¤±è´¥ä¸å½±å“å…¶ä»–"""
    results = []
    for i, future in enumerate(futures):
        try:
            result = await future
            results.extend(result)
        except Exception as e:
            self.logger.error(f"Producer group {i} failed: {e}")
            # âœ… åˆ›å»ºé”™è¯¯ç»“æœå ä½
            results.append(self._create_error_result('producer', e))
    return results
```

#### é—®é¢˜ 11: Worker å¤±è´¥æ¢å¤æœºåˆ¶ç¼ºå¤±

**å½“å‰å®ç°**:
```python
# coordinator.py:137-161
async def _check_worker_health(self):
    tasks = []
    for worker_url in self.config.workers:
        tasks.append(self._check_single_worker_health(worker_url))

    health_results = await asyncio.gather(*tasks, return_exceptions=True)

    healthy_workers = []
    for i, result in enumerate(health_results):
        if isinstance(result, Exception):
            self.logger.error(f"Worker {worker_url} health check failed")
        else:
            healthy_workers.append(worker_url)

    # âœ… è¿‡æ»¤æ‰ä¸å¥åº·çš„ Worker
    self.config.workers = healthy_workers

    # âŒ ä½†ä»»åŠ¡å·²ç»åˆ†å‘å®Œæ¯•, æ— æ³•é‡æ–°åˆ†é…
```

**é—®é¢˜åœºæ™¯**:
```python
åœºæ™¯: 3 ä¸ª Worker, åˆå§‹å¥åº·æ£€æŸ¥é€šè¿‡

t=0s:  å¥åº·æ£€æŸ¥ âœ… Worker 1, 2, 3 å…¨éƒ¨å¥åº·
t=1s:  ä»»åŠ¡åˆ†å‘
       â”œâ”€â”€ Worker 1: 33 ä¸ª Producer ä»»åŠ¡
       â”œâ”€â”€ Worker 2: 33 ä¸ª Producer ä»»åŠ¡
       â””â”€â”€ Worker 3: 34 ä¸ª Producer ä»»åŠ¡

t=5s:  Worker 3 å´©æºƒ âŒ
       â”œâ”€â”€ 34 ä¸ª Producer ä»»åŠ¡ä¸¢å¤±
       â”œâ”€â”€ Coordinator ç­‰å¾… Worker 3 å“åº”
       â”œâ”€â”€ æœ€ç»ˆè¶…æ—¶ (å¦‚æœæœ‰è¶…æ—¶æœºåˆ¶)
       â””â”€â”€ æµ‹è¯•ç»“æœä¸å®Œæ•´

ç†æƒ³è¡Œä¸º:
â”œâ”€â”€ æ£€æµ‹åˆ° Worker 3 å¤±è´¥
â”œâ”€â”€ å°†å…¶ 34 ä¸ªä»»åŠ¡é‡æ–°åˆ†é…ç»™ Worker 1, 2
â””â”€â”€ æµ‹è¯•ç»§ç»­è¿›è¡Œ
```

**åŸç‰ˆ OMB (Java)**:
```java
// WorkloadGenerator.java
public class WorkloadGenerator {
    private final WorkerPool workerPool;

    public void runTest() {
        // ä»»åŠ¡åˆ†å‘æ—¶åŠ¨æ€åˆ†é…
        for (ProducerTask task : tasks) {
            Worker worker = workerPool.getHealthyWorker();  // åŠ¨æ€è·å–

            CompletableFuture<ProducerResult> future =
                worker.runProducer(task)
                    .exceptionally(ex -> {
                        // Worker å¤±è´¥, é‡è¯•å…¶ä»– Worker
                        Worker fallbackWorker = workerPool.getHealthyWorker();
                        return fallbackWorker.runProducer(task).join();
                    });

            futures.add(future);
        }
    }
}
```

**æ­£ç¡®åšæ³•**:
```python
class WorkerPool:
    """Worker æ± , æ”¯æŒåŠ¨æ€å¥åº·æ£€æŸ¥å’Œæ•…éšœè½¬ç§»"""

    def __init__(self, worker_urls: List[str]):
        self.workers = {url: {'url': url, 'healthy': True, 'load': 0}
                       for url in worker_urls}
        self._lock = asyncio.Lock()

    async def get_healthy_worker(self) -> str:
        """è·å–å¥åº·çš„ Worker (è´Ÿè½½å‡è¡¡)"""
        async with self._lock:
            healthy = [w for w in self.workers.values() if w['healthy']]
            if not healthy:
                raise RuntimeError("No healthy workers available")

            # é€‰æ‹©è´Ÿè½½æœ€ä½çš„
            worker = min(healthy, key=lambda w: w['load'])
            worker['load'] += 1
            return worker['url']

    async def mark_unhealthy(self, worker_url: str):
        """æ ‡è®° Worker ä¸å¥åº·"""
        async with self._lock:
            if worker_url in self.workers:
                self.workers[worker_url]['healthy'] = False

    async def periodic_health_check(self):
        """åå°å®šæœŸå¥åº·æ£€æŸ¥"""
        while True:
            await asyncio.sleep(10)
            for url, info in self.workers.items():
                try:
                    await self._check_health(url)
                    info['healthy'] = True
                except Exception:
                    info['healthy'] = False

async def _run_worker_producer_tasks_with_retry(
    self,
    worker_url: str,
    tasks: List[ProducerTask],
    max_retries: int = 2
) -> List[WorkerResult]:
    """å¸¦é‡è¯•çš„ä»»åŠ¡æ‰§è¡Œ"""
    for attempt in range(max_retries + 1):
        try:
            return await self._run_worker_producer_tasks(worker_url, tasks)

        except Exception as e:
            self.logger.warning(
                f"Worker {worker_url} failed (attempt {attempt + 1}/{max_retries + 1}): {e}"
            )

            if attempt < max_retries:
                # æ ‡è®°ä¸å¥åº·
                await self.worker_pool.mark_unhealthy(worker_url)

                # è·å–æ–°çš„å¥åº· Worker
                worker_url = await self.worker_pool.get_healthy_worker()
                self.logger.info(f"Retrying with worker {worker_url}")
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥, è¿”å›é”™è¯¯ç»“æœ
                return [self._create_error_result('producer', e, tasks)]
```

### 2.6 èµ„æºç®¡ç†é—®é¢˜

#### é—®é¢˜ 12: Topic æ¸…ç†ä¸å¯é 

**å½“å‰å®ç°**:
```python
# coordinator.py:531-595
async def _cleanup_test_topics(self, driver_config):
    """æ¸…ç†æµ‹è¯• Topics"""
    if not hasattr(self, '_current_test_topics'):
        return

    try:
        # âŒ ä¸ºæ¸…ç†ä¸´æ—¶åˆ›å»ºæ–°çš„ Driver
        driver = driver_class(driver_config)
        await driver.initialize()

        try:
            topic_manager = driver.create_topic_manager()

            for topic_name in self._current_test_topics:
                try:
                    await topic_manager.delete_topic(topic_name)
                except Exception as e:
                    # âš ï¸ åˆ é™¤å¤±è´¥ä»…è®°å½•è­¦å‘Š
                    self.logger.warning(f"Failed to delete {topic_name}: {e}")

        finally:
            await driver.cleanup()

    except Exception as e:
        # âš ï¸ æ•´ä¸ªæ¸…ç†å¤±è´¥ä»…è®°å½•è­¦å‘Š
        self.logger.warning(f"Topic cleanup failed: {e}")

    finally:
        self._current_test_topics = []  # æ¸…ç©ºåˆ—è¡¨
```

**é—®é¢˜åˆ†æ**:

**é—®é¢˜ 1: Driver å®ä¾‹æ··ä¹±**
```python
æµ‹è¯•æ‰§è¡Œæ—¶:
â”œâ”€â”€ Coordinator åˆ›å»º Driver A (ç”¨äº setup topics)
â”œâ”€â”€ Worker 1 åˆ›å»º Driver B (ç”¨äºæ‰§è¡Œä»»åŠ¡)
â”œâ”€â”€ Worker 2 åˆ›å»º Driver C (ç”¨äºæ‰§è¡Œä»»åŠ¡)
â””â”€â”€ Coordinator åˆ›å»º Driver D (ç”¨äº cleanup topics)

é—®é¢˜:
- 4 ä¸ªç‹¬ç«‹çš„ Driver å®ä¾‹
- æ¯ä¸ªéƒ½ç»´æŠ¤ç‹¬ç«‹çš„è¿æ¥
- é…ç½®å¯èƒ½ä¸ä¸€è‡´
- èµ„æºæµªè´¹
```

**é—®é¢˜ 2: æ¸…ç†å¤±è´¥è¢«å¿½ç•¥**
```python
åœºæ™¯: Topic åˆ é™¤å¤±è´¥

å¯èƒ½åŸå› :
â”œâ”€â”€ Kafka broker æš‚æ—¶ä¸å¯ç”¨
â”œâ”€â”€ Topic è¿˜æœ‰æ´»è·ƒçš„ Consumer
â”œâ”€â”€ Replication è¿˜åœ¨è¿›è¡Œä¸­
â””â”€â”€ æƒé™ä¸è¶³

å½“å‰è¡Œä¸º:
â”œâ”€â”€ è®°å½•è­¦å‘Šæ—¥å¿—
â”œâ”€â”€ ç»§ç»­æ‰§è¡Œåç»­æµ‹è¯•
â””â”€â”€ Topic æ®‹ç•™åœ¨ Kafka ä¸­

å½±å“:
â”œâ”€â”€ ç£ç›˜ç©ºé—´å ç”¨å¢åŠ 
â”œâ”€â”€ åç»­æµ‹è¯•å¯èƒ½å—å½±å“ (å¦‚æœ topic åç§°å†²çª)
â””â”€â”€ Kafka æ€§èƒ½ä¸‹é™ (å¤§é‡æ®‹ç•™ topic)
```

**é—®é¢˜ 3: æ¸…ç†æ—¶æœºä¸å½“**
```python
å½“å‰æ—¶æœº: æµ‹è¯•ç»“æŸåç«‹å³æ¸…ç†

é—®é¢˜:
â”œâ”€â”€ Producer å¯èƒ½è¿˜æœ‰æ¶ˆæ¯åœ¨ç¼“å†²åŒº
â”œâ”€â”€ Consumer å¯èƒ½è¿˜åœ¨æ¶ˆè´¹æœ€åçš„æ¶ˆæ¯
â”œâ”€â”€ Kafka å¯èƒ½è¿˜åœ¨æ‰§è¡Œ replication
â””â”€â”€ ç«‹å³åˆ é™¤å¯èƒ½å¯¼è‡´é”™è¯¯

ç†æƒ³æ—¶æœº:
â”œâ”€â”€ ç­‰å¾…æ‰€æœ‰ Producer flush å®Œæˆ
â”œâ”€â”€ ç­‰å¾…æ‰€æœ‰ Consumer commit å®Œæˆ
â”œâ”€â”€ ç­‰å¾… Kafka replication å®Œæˆ (å¯é€‰)
â””â”€â”€ å†æ‰§è¡Œåˆ é™¤
```

**åŸç‰ˆ OMB çš„åšæ³•**:
```java
// Cleanup æ˜¯å¯é€‰çš„, é»˜è®¤ä¸æ¸…ç†
public void cleanup() {
    if (config.shouldCleanup()) {  // é»˜è®¤ false
        try {
            // ç­‰å¾…ä¸€æ®µæ—¶é—´ç¡®ä¿æ¶ˆæ¯å¤„ç†å®Œæ¯•
            Thread.sleep(5000);

            // åˆ é™¤ topics
            adminClient.deleteTopics(topicNames).all().get(60, TimeUnit.SECONDS);

        } catch (Exception e) {
            // æ¸…ç†å¤±è´¥ä¸å½±å“æµ‹è¯•ç»“æœ
            log.warn("Cleanup failed, topics may remain: {}", topicNames, e);
        }
    } else {
        log.info("Cleanup disabled, topics will remain: {}", topicNames);
    }
}
```

**æ­£ç¡®åšæ³•**:
```python
class BenchmarkCoordinator:
    def __init__(self, config):
        self.config = config
        # âœ… ç»Ÿä¸€çš„ Driver å®ä¾‹
        self._driver: Optional[AbstractDriver] = None

    async def run_benchmark(self, workload_config, driver_config, ...):
        # âœ… åœ¨æµ‹è¯•å¼€å§‹æ—¶åˆ›å»º Driver
        self._driver = self._create_driver(driver_config)
        await self._driver.initialize()

        try:
            # åˆ›å»º topics
            await self._setup_topics(workload_config)

            # æ‰§è¡Œæµ‹è¯•
            await self._run_test_phase(...)

        finally:
            # æ¸…ç†èµ„æº
            try:
                if self.config.cleanup_enabled:
                    await self._cleanup_topics_safely()
            finally:
                # âœ… ç¡®ä¿ Driver å…³é—­
                if self._driver:
                    await self._driver.cleanup()

    async def _cleanup_topics_safely(self):
        """å®‰å…¨æ¸…ç† Topics"""
        if not self._current_test_topics:
            return

        self.logger.info(f"Cleaning up {len(self._current_test_topics)} topics...")

        # âœ… ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæ¯•
        self.logger.info("Waiting for messages to settle...")
        await asyncio.sleep(5)

        # âœ… ä½¿ç”¨ç»Ÿä¸€çš„ Driver
        topic_manager = self._driver.create_topic_manager()

        failed_topics = []
        for topic_name in self._current_test_topics:
            try:
                # âœ… å¸¦è¶…æ—¶çš„åˆ é™¤
                await asyncio.wait_for(
                    topic_manager.delete_topic(topic_name),
                    timeout=30
                )
                self.logger.debug(f"Deleted topic: {topic_name}")

            except asyncio.TimeoutError:
                self.logger.error(f"Timeout deleting topic: {topic_name}")
                failed_topics.append(topic_name)

            except Exception as e:
                self.logger.error(f"Failed to delete topic {topic_name}: {e}")
                failed_topics.append(topic_name)

        # âœ… æ¸…ç†ç»“æœæŠ¥å‘Š
        if failed_topics:
            self.logger.warning(
                f"Failed to cleanup {len(failed_topics)} topics: {failed_topics}\n"
                f"Please manually delete them: kafka-topics --delete --topic <name>"
            )
        else:
            self.logger.info("âœ… All topics cleaned up successfully")
```

---

## ä¸‰ã€æ”¹è¿›å»ºè®®ä¸ä¼˜å…ˆçº§

### 3.1 Critical (å¿…é¡»ä¿®å¤)

#### 1. ä¿®å¤ Consumer é…ç½®é”™è¯¯
**é—®é¢˜**: `auto.offset.reset=latest` å¯¼è‡´æ¶ˆæ¯ä¸¢å¤±
**å½±å“**: æµ‹è¯•ç»“æœå®Œå…¨ä¸å‡†ç¡®
**ä¿®å¤**:
```yaml
# æ‰€æœ‰é…ç½®æ–‡ä»¶
consumerConfig: |
  auto.offset.reset=earliest  # âœ… æ”¹ä¸º earliest
```
**å·¥ä½œé‡**: 10 åˆ†é’Ÿ
**ä¼˜å…ˆçº§**: â­â­â­â­â­

#### 2. ä¿®å¤é…ç½®ç±»å‹è½¬æ¢
**é—®é¢˜**: æ‰€æœ‰é…ç½®å€¼éƒ½æ˜¯å­—ç¬¦ä¸²
**å½±å“**: Kafka å‚æ•°å¯èƒ½ä¸ç”Ÿæ•ˆ
**ä¿®å¤**: å®ç° `parse_config_with_type_conversion()`
**å·¥ä½œé‡**: 2 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­â­

#### 3. ä¿®å¤å»¶è¿Ÿç»Ÿè®¡èšåˆ
**é—®é¢˜**: åˆ†ä½æ•°è®¡ç®—é”™è¯¯ (å–æœ€å¤§å€¼)
**å½±å“**: å»¶è¿ŸæŒ‡æ ‡ä¸¥é‡åç¦»çœŸå®å€¼
**ä¿®å¤**: åˆå¹¶ HdrHistogram, ä¿ç•™åŸå§‹ç›´æ–¹å›¾
**å·¥ä½œé‡**: 4 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­â­

#### 4. ä¿®å¤ Producer å¼‚æ­¥æ¨¡å‹
**é—®é¢˜**: è¿‡åº¦åŒ…è£… `confluent-kafka` çš„å¼‚æ­¥ API
**å½±å“**: æ€§èƒ½æŸå¤± 20-30%
**ä¿®å¤**: ç§»é™¤ `run_in_executor`, ç›´æ¥è°ƒç”¨ `produce()`
**å·¥ä½œé‡**: 3 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­

### 3.2 High (å¼ºçƒˆå»ºè®®)

#### 5. å®ç° Consumer å°±ç»ªåŒæ­¥æœºåˆ¶
**é—®é¢˜**: `sleep(5)` ä¸å¯é 
**å½±å“**: å¯èƒ½ä¸¢å¤±æ¶ˆæ¯, ååé‡ä¸å‡†
**ä¿®å¤**:
- Worker API å¢åŠ  `/consumer/{id}/ready` ç«¯ç‚¹
- Coordinator è½®è¯¢ç­‰å¾…æ‰€æœ‰ Consumer å°±ç»ª
**å·¥ä½œé‡**: 6 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­

#### 6. å®Œå–„ Producer å»¶è¿Ÿç»Ÿè®¡
**é—®é¢˜**: å›è°ƒæœªå®Œæˆå°±è·å–ç»Ÿè®¡
**å½±å“**: å»¶è¿Ÿæ ·æœ¬ç¼ºå¤±
**ä¿®å¤**:
- ä½¿ç”¨ CountdownLatch ç­‰å¾…æ‰€æœ‰å›è°ƒ
- å¢åŠ è¶…æ—¶ä¿æŠ¤
**å·¥ä½œé‡**: 4 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­

#### 7. ç§»é™¤æ— æ•ˆçš„è¿æ¥æ± 
**é—®é¢˜**: è¿æ¥æ± æœªçœŸæ­£å¤ç”¨
**å½±å“**: ä»£ç å¤æ‚åº¦é«˜, å†…å­˜æµªè´¹
**ä¿®å¤**:
- åˆ é™¤ `_initialize_pools()` ç›¸å…³ä»£ç 
- æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹åˆ›å»º/é”€æ¯è¿æ¥
**å·¥ä½œé‡**: 2 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­

#### 8. å¢åŠ è¶…æ—¶æœºåˆ¶
**é—®é¢˜**: Worker å¤±è´¥ä¼šå¯¼è‡´æ°¸ä¹…é˜»å¡
**å½±å“**: æµ‹è¯•å¯é æ€§å·®
**ä¿®å¤**:
- æ‰€æœ‰ Worker è°ƒç”¨å¢åŠ è¶…æ—¶ (æµ‹è¯•æ—¶é—´ + 120s)
- éƒ¨åˆ†å¤±è´¥ä¸å½±å“ç»“æœæ”¶é›†
**å·¥ä½œé‡**: 4 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­â­

### 3.3 Medium (å»ºè®®ä¼˜åŒ–)

#### 9. ä¼˜åŒ–å¤šè¿›ç¨‹å‘é€å™¨
**é—®é¢˜**: å¤šè¿›ç¨‹åè€Œé™ä½æ€§èƒ½
**å½±å“**: é«˜åååœºæ™¯ä¸å¦‚é¢„æœŸ
**ä¿®å¤**:
- ç§»é™¤å¤šè¿›ç¨‹é€»è¾‘ (confluent-kafka å·²ç»•è¿‡ GIL)
- ä¼˜åŒ–æ‰¹é‡å‘é€å‚æ•°
**å·¥ä½œé‡**: 4 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­

#### 10. å®ç° Worker æ•…éšœè½¬ç§»
**é—®é¢˜**: Worker å¤±è´¥å¯¼è‡´ä»»åŠ¡ä¸¢å¤±
**å½±å“**: å¤§è§„æ¨¡æµ‹è¯•å¯é æ€§å·®
**ä¿®å¤**:
- å®ç° WorkerPool åŠ¨æ€å¥åº·æ£€æŸ¥
- ä»»åŠ¡å¤±è´¥è‡ªåŠ¨é‡è¯•åˆ°å…¶ä»– Worker
**å·¥ä½œé‡**: 8 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­

#### 11. ä¼˜åŒ–ååé‡èšåˆ
**é—®é¢˜**: ä½¿ç”¨æœ€å¤§æŒç»­æ—¶é—´è®¡ç®—ååé‡
**å½±å“**: ååé‡å¯èƒ½åä½
**ä¿®å¤**: ä½¿ç”¨å®é™…å¹¶å‘æ—¶é—´çª—å£ (min_start_time åˆ° max_end_time)
**å·¥ä½œé‡**: 2 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­â­

#### 12. æ”¹è¿› Topic æ¸…ç†æœºåˆ¶
**é—®é¢˜**: æ¸…ç†å¤±è´¥è¢«å¿½ç•¥
**å½±å“**: Topic æ®‹ç•™
**ä¿®å¤**:
- ç»Ÿä¸€ Driver å®ä¾‹ç®¡ç†
- ç­‰å¾…æ¶ˆæ¯å¤„ç†å®Œæ¯•å†æ¸…ç†
- æ¸…ç†å¤±è´¥æä¾›æ˜ç¡®æŒ‡å¼•
**å·¥ä½œé‡**: 3 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­

### 3.4 Low (å¯é€‰ä¼˜åŒ–)

#### 13. å¢åŠ ç»“æœéªŒè¯
**é—®é¢˜**: æ— æ³•æ£€æµ‹å¼‚å¸¸ç»“æœ
**å½±å“**: é”™è¯¯ç»“æœè¢«å½“ä½œæ­£ç¡®ç»“æœ
**ä¿®å¤**:
- Producer/Consumer æ¶ˆæ¯æ•°åŒ¹é…æ£€æŸ¥
- å»¶è¿Ÿå¼‚å¸¸å€¼æ£€æµ‹
- ååé‡åˆç†æ€§æ£€æŸ¥
**å·¥ä½œé‡**: 4 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­

#### 14. ä¼˜åŒ–æ—¥å¿—è¾“å‡º
**é—®é¢˜**: æ—¥å¿—è¿‡äºè¯¦ç»†æˆ–ä¸å¤Ÿè¯¦ç»†
**å½±å“**: éš¾ä»¥å®šä½é—®é¢˜
**ä¿®å¤**:
- ç»“æ„åŒ–æ—¥å¿— (JSON æ ¼å¼)
- åˆ†çº§æ—¥å¿—æ§åˆ¶
- å…³é”®æŒ‡æ ‡å®æ—¶è¾“å‡º
**å·¥ä½œé‡**: 3 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­

#### 15. å¢åŠ å•å…ƒæµ‹è¯•
**é—®é¢˜**: ç¼ºå°‘æµ‹è¯•è¦†ç›–
**å½±å“**: é‡æ„é£é™©é«˜
**ä¿®å¤**:
- æ ¸å¿ƒæ¨¡å—å•å…ƒæµ‹è¯• (config, results, coordinator)
- é›†æˆæµ‹è¯• (ç«¯åˆ°ç«¯æµ‹è¯•)
**å·¥ä½œé‡**: 12 å°æ—¶
**ä¼˜å…ˆçº§**: â­â­

---

## å››ã€é‡æ„è·¯çº¿å›¾

### Phase 1: ä¿®å¤å…³é”®é”™è¯¯ (1 å‘¨)
```
1. ä¿®å¤ Consumer é…ç½® (auto.offset.reset)
2. ä¿®å¤é…ç½®ç±»å‹è½¬æ¢
3. ä¿®å¤å»¶è¿Ÿç»Ÿè®¡èšåˆ
4. ä¿®å¤ Producer å¼‚æ­¥æ¨¡å‹
5. å¢åŠ åŸºæœ¬çš„è¶…æ—¶æœºåˆ¶
```

**é¢„æœŸæ•ˆæœ**:
- æµ‹è¯•ç»“æœå‡†ç¡®æ€§æå‡ 80%
- æ€§èƒ½æå‡ 20-30%
- åŸºæœ¬çš„å®¹é”™èƒ½åŠ›

### Phase 2: æ”¹è¿›æ—¶åºæ§åˆ¶ (1 å‘¨)
```
1. å®ç° Consumer å°±ç»ªåŒæ­¥
2. å®Œå–„ Producer å»¶è¿Ÿç»Ÿè®¡
3. ç§»é™¤æ— æ•ˆè¿æ¥æ± 
4. ä¼˜åŒ–ååé‡èšåˆ
```

**é¢„æœŸæ•ˆæœ**:
- æµ‹è¯•å¯é æ€§æå‡ 90%
- æ¶ˆé™¤æ—¶åºç›¸å…³é—®é¢˜
- ä»£ç ç®€åŒ– 20%

### Phase 3: å¢å¼ºå¯é æ€§ (1-2 å‘¨)
```
1. å®ç° Worker æ•…éšœè½¬ç§»
2. æ”¹è¿› Topic æ¸…ç†æœºåˆ¶
3. å¢åŠ ç»“æœéªŒè¯
4. å®Œå–„é”™è¯¯å¤„ç†
```

**é¢„æœŸæ•ˆæœ**:
- æ”¯æŒå¤§è§„æ¨¡åˆ†å¸ƒå¼æµ‹è¯•
- Worker æ•…éšœè‡ªåŠ¨æ¢å¤
- å¼‚å¸¸ç»“æœè‡ªåŠ¨æ£€æµ‹

### Phase 4: ä¼˜åŒ–ä¸æµ‹è¯• (1 å‘¨)
```
1. æ€§èƒ½ä¼˜åŒ– (ç§»é™¤å¤šè¿›ç¨‹)
2. å¢åŠ å•å…ƒæµ‹è¯•
3. æ”¹è¿›æ—¥å¿—ç³»ç»Ÿ
4. ç¼–å†™æ–‡æ¡£
```

**é¢„æœŸæ•ˆæœ**:
- æ€§èƒ½æ¥è¿‘ Java OMB
- æµ‹è¯•è¦†ç›–ç‡ > 60%
- å®Œå–„çš„æ–‡æ¡£

---

## äº”ã€ä¸åŸç‰ˆ OMB çš„è®¾è®¡å“²å­¦å¯¹æ¯”

### åŸç‰ˆ OMB (Java) çš„è®¾è®¡å“²å­¦

1. **ç®€å•ç›´æ¥**:
   - æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹çš„è¿æ¥
   - æ¸…æ™°çš„åŒæ­¥æœºåˆ¶ (CountdownLatch)
   - æœ€å°åŒ–æŠ½è±¡å±‚æ¬¡

2. **ç²¾ç¡®å¯é **:
   - ç²¾ç¡®çš„å»¶è¿Ÿæµ‹é‡ (HdrHistogram)
   - å®Œæ•´çš„å›è°ƒç­‰å¾…æœºåˆ¶
   - ä¸¥æ ¼çš„ç»“æœéªŒè¯

3. **å®¹é”™ä¼˜å…ˆ**:
   - å®Œå–„çš„è¶…æ—¶æœºåˆ¶
   - Worker å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
   - æ¸…æ™°çš„é”™è¯¯ä¼ æ’­

### å½“å‰ Python å®ç°çš„é—®é¢˜

1. **è¿‡åº¦å·¥ç¨‹åŒ–**:
   - ä¸å¿…è¦çš„è¿æ¥æ± 
   - å¤æ‚çš„å¤šè¿›ç¨‹æ¶æ„
   - è¿‡åº¦çš„å¼‚æ­¥åŒ…è£…

2. **ç²¾ç¡®åº¦ä¸è¶³**:
   - ä¸å‡†ç¡®çš„ç»Ÿè®¡èšåˆ
   - å›è°ƒæœªå®Œæˆå°±æ”¶é›†ç»“æœ
   - ç¼ºå°‘ç»“æœéªŒè¯

3. **å®¹é”™ç¼ºå¤±**:
   - æ— è¶…æ—¶æœºåˆ¶
   - Worker å¤±è´¥å¯¼è‡´é˜»å¡
   - æ¸…ç†å¤±è´¥è¢«å¿½ç•¥

### å»ºè®®çš„è®¾è®¡åŸåˆ™

1. **Keep It Simple**:
   - ç§»é™¤ä¸å¿…è¦çš„æŠ½è±¡ (è¿æ¥æ± ã€å¤šè¿›ç¨‹)
   - ç›´æ¥ä½¿ç”¨ confluent-kafka çš„å¼‚æ­¥ API
   - æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹çš„ç”Ÿå‘½å‘¨æœŸ

2. **Correctness First**:
   - å‡†ç¡®çš„ç»Ÿè®¡èšåˆ (åˆå¹¶ç›´æ–¹å›¾)
   - å®Œæ•´çš„å›è°ƒç­‰å¾…
   - ä¸¥æ ¼çš„ç»“æœéªŒè¯

3. **Fail Fast and Recover**:
   - å®Œå–„çš„è¶…æ—¶æœºåˆ¶
   - Worker æ•…éšœè½¬ç§»
   - æ¸…æ™°çš„é”™è¯¯æŠ¥å‘Š

---

## å…­ã€æ€»ç»“

### æ ¸å¿ƒé—®é¢˜

1. **æ¶æ„å±‚é¢**: å¼‚æ­¥æ¨¡å‹æ··ä¹±, è¿æ¥æ± è¿‡åº¦è®¾è®¡, å¤šè¿›ç¨‹é”™è¯¯å‡è®¾
2. **æ—¶åºæ§åˆ¶**: Consumer åŒæ­¥æœºåˆ¶ç¼ºå¤±, å»¶è¿Ÿç»Ÿè®¡æ—¶æœºé”™è¯¯
3. **ç»Ÿè®¡èšåˆ**: åˆ†ä½æ•°è®¡ç®—é”™è¯¯, ååé‡è®¡ç®—ä¸å‡†ç¡®
4. **é…ç½®å¤„ç†**: ç±»å‹è½¬æ¢ç¼ºå¤±, Consumer é…ç½®è‡´å‘½é”™è¯¯
5. **å¯é æ€§**: è¶…æ—¶æœºåˆ¶ç¼ºå¤±, é”™è¯¯å¤„ç†ä¸è¶³

### æ ¹æœ¬åŸå› 

è¯•å›¾ç”¨ Python çš„å¼‚æ­¥æ¨¡å‹ç›´æ¥ç¿»è¯‘ Java çš„å¹¶å‘æ¨¡å‹ï¼Œä½†:
- æ²¡æœ‰ç†è§£ confluent-kafka çš„å¼‚æ­¥æœ¬è´¨ (C æ‰©å±•, æ—  GIL)
- æ²¡æœ‰ç†è§£åŸç‰ˆ OMB çš„ç®€æ´è®¾è®¡å“²å­¦
- è¿‡åº¦è¿½æ±‚"ä¼˜åŒ–" (è¿æ¥æ± ã€å¤šè¿›ç¨‹), åè€Œå¼•å…¥å¤æ‚åº¦
- å¿½è§†äº†åŸºå‡†æµ‹è¯•çš„æ ¸å¿ƒ: **ç²¾ç¡®æ€§å’Œå¯é æ€§**

### æ”¹è¿›æ–¹å‘

**çŸ­æœŸ (1-2 å‘¨)**:
- ä¿®å¤å…³é”®é”™è¯¯ (é…ç½®ã€ç»Ÿè®¡ã€å¼‚æ­¥)
- å»ºç«‹åŸºæœ¬çš„å®¹é”™èƒ½åŠ›

**ä¸­æœŸ (3-4 å‘¨)**:
- æ”¹è¿›æ—¶åºæ§åˆ¶
- å¢å¼ºå¯é æ€§
- ç®€åŒ–æ¶æ„

**é•¿æœŸ**:
- æ€§èƒ½ä¼˜åŒ–
- å®Œå–„æµ‹è¯•
- å¯¹é½åŸç‰ˆ OMB åŠŸèƒ½

### æœ€åçš„å»ºè®®

**ä¸è¦ç›²ç›®è¿½æ±‚ Python ç‰¹è‰²**, åŸºå‡†æµ‹è¯•æ¡†æ¶çš„æ ¸å¿ƒæ˜¯:
1. å‡†ç¡®çš„æ€§èƒ½æµ‹é‡
2. å¯é çš„æµ‹è¯•æ‰§è¡Œ
3. ç®€å•çš„æ¶æ„è®¾è®¡

å»ºè®®å‚è€ƒåŸç‰ˆ OMB çš„è®¾è®¡æ€è·¯, å°† Java çš„ç®€æ´æ€§ç§»æ¤åˆ° Python, è€Œä¸æ˜¯å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-09-30
**ä½œè€…**: Design Analysis Team
